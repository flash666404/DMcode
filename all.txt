For? Okay, thats enough.

For that.

Good afternoon, everyone. I'm thank you. I'm pleasantly surprised that you've all decided to come.

Along.

I did say this course is largely being taught via recordings of online lectures. And each week there are 3 or 2 lectures to watch and you can see the PowerPoint slides and the transcripts to figure out what theyre about. The lecture sort of give a summary of a topic area. Theres more details if you read the background reading that.

Comes with it.

And theres also some sort of exercises to do. Last week it was to have a go using sketch engine to collect the corpus and do some interesting analysis. This week I'm asking you to have a go at playing with weccer is like an alternative way of doing machine learning and data binding. Probably youve had to go writing some code in python to do data mining or machine learning and most computer scientists seem to like doing it in python. But a reasonable alternative if particularly if you got to go and work in industry, if you may be asked to use some sort of toolkit with drop down menus and drag and drop or something like rapid, minor or weaker. So I think at least you should have a go at using this and to try to work out in your own mind, what are the advantages and disadvantages of using a toolkit like record as compared to doing it in python. And obviously in weccer you don't have to do any coding, which means you just have to click on, load up the file and then click on which classify you want to use and out comes some results. So its in some sense easier to use.

On the other hand, youre constrained to what is available, whereas if youre writing python code that you choose what it is you want to do. So youre more flexible in python, but im just asking you to have a go at using this. Im not going to be marking, giving a grade for this.

As I said before, the grading for this module is going to be a test in week 4 and another test at the end of the course, 8 I think it is, which will be 20 multiple choice questions online and you have to just sit by your computer yourself and answer the questions nice and straightforwardly. It'll probably, I could probably do it in one minute, you could probably do it in a few minutes, but you've got an hour to do it in, there will be a 24 hour window and you can start any time within that 24 hour window. Or advise you dont wait until the very end just because you, if you have an internet problem then you've missed your charts, right? So make sure you do it reasonably.

Early on.

There is the assessment specification here, which tells you more about this. The final piece of coursework, ill be talking about this in some more detail later on, but essentially I wanted to come up with a research proposal for a nice, interesting research project, and I wanted to work in a group of 4:6 people. If you really can't work in a group for some reason, then email me and we'll sort something out instead. But in industry, people do work in teams, so its a good idea, not just the Microsoft teams, but as a group of people. So its a good idea to get some sort of experience of doing this. And also it means theres less work for you each individually and also for coming up with a research proposal that's quite a difficult thing to do and it often helps to discuss your ideas with other people to come up with a a plausible proposal now this could be something. To do for your MSC project so its up to you what scale it is it could be what a project for one person over 6 months or for a group of 4 people over 6 months or it could be im trying to put together a research grant proposal to. The engineering and physical science research council for a 3 or 4 year project involving 3 or 4 people so it could be that sort of scale if you want to that is obviously harder to make it plausible and it should you have to make sure you make use of, at least 2 of the technologies were covering in this course things like chatbots or gpt or Bert or wacker or sketch engine, one of those sorts of things or nlt casing at least 2 of them.

So don't.

It cant just be any old research project. Its going to be something which makes use of data lining and text analytics technologies but it could be something that youre interested in either your job or your hobby or it could be about education at university so you might have use of chat gtp for teaching computer science to. University students as an ice topic that you might.

Have for a project.

Yes, ill tell you some more about thats more about in detail later on, I think in week 4. But for now, just we'll go back.

To.

What we have before. But however, theres one thing ID like you to have a look at.

Now.

Close this up. I've put in an excel spreadsheet or a link to an excel spreadsheet, which anybody can write to. Please do not mess this up because anybody can write to it. I don't want you to mess it up. All you ask you to do is find your name and your email address. Make sure it is the right name and.

Email address.

And then next to in column c at the name of your.

Group.

Or team your group. Right. That way I can that way I can see.

That all.

I can see the 4 people with the same group name belong to the same.

Group.

Now notice that sub. There is one group that's already done this. Alexander Lena montes. Is Alexander here. No. Okay. Alexander is probably watching at home on the recording afterwards. So his Alexander has put essential 6 as the group name. That's what I wanted. But he's put in the other members of the team here. There's one 23,4,5,6. But it should only be, I did say if you include Alexander as well, that makes 7. Thats too many, right? I did say 4:6. And furthermore, I don't want the names here. I want boshi Wu. Where is bushi rule? That's probably the people in here. Boshi woo. Have we got woo boshi here? Okay. Woo.

Boshi. Oh, hello.

You were supposed to put the name of the group here, right? Right. Please, each of you, all these.

People.

Put your name of your group in column c. Don't put the other names. I don't want d, e, f, g, h filling up all.

The other things.

I just want that this is so that other people can see who hasnt got a group so at the moment, the everybody hasnt got a group, but if you can see who else hasnt got a group, then you can go and approach them. Youve got their email address, you can ask them, can you, would you like to join in the group with me? Okay, does that make.

Sense?

Anybody any questions about this? Its not rocket science. It shouldnt be complicated, but it just shows if you have a shareable data file, then people do mess it up and this is part of the learning curve for data mining is the first thing you have to do is get hold of some data and get it into the right format so that then you can do some processing. So already we haven't got this in the right format. And what you have to do if this was an actual data mining exercise is you'd have to go in, take those names and take the as team name and put it somewhere else in there. Okay.

That's that.

Thats the assessment. Why want to do it for the rest of.

Today.

Is to have a look at unit2.

Hello, are we setting up teams amongst yourselves? Youre thinking about it? Okay, so unit 2 is about text classification and data mining, open resources and weccer. Ive got 3 recorded lectures for you.

To watch.

I'm going to quickly summarize the high points the rationale being so if you want to, you dont have to watch these you could just watch this final summary lecture and because im going to tell you the main points you have to know and you have to think about. However, if you want to do well in the test then I would watch the recordings because the test will have questions about this stuff. And if you want to become if you want a distinction grade then watch the recordings and also do the background reading because feature is, there is a either a chapter of a textbook or some other reading for.

You to do.

So the slides that look like this. These are directly from James, Martin and dan, the office of the textbook. And so I've, I've borrowed this, I've changed a few bits and pieces just to make it appropriate for leaves, because theyve got lots of American examples and I want to have British examples. Okay, so language modeling. So the idea of a language model is to have some way of working out for a given sentence or a given. Piece of text, its probability compared to other pieces of text. Why would you want to do this? Well, in lots of applications, there are more than one candidate. If I'm translating from some language like, say Arabic or Spanish into English, and you got to and and that the Arabic original is something about very strong or very strong winds are blowing tonight. Then in English you might say high winds tonight but you dont say large winds tonight. Even though high and large mean the same thing in some sense. For wins you have to have high rather than large or big. You don't say big winds tonight, its high winds tonight or possibly strong winds tonight.

A spelling correction, if you have a Microsoft word it does a sort of squiggly underlying if what you type in isnt in its dictionary. However, if what you type in is in its dictionary but it doesnt look right in the context, because it is an English word but it's just not the right word, but it does a green underline. And so if let's say about 15 minutes from, that's OK, whereas from a mini x is an English word, a mini way is a piece of music, but its in the context of about 15 something from its more likely to be minutes. So you have to be able to calculate the probability of both of those and then work out which is the most likely. And the same thing goes in speech recognition. If I if its a speech signal which sounds like I saw a.

Van.

Then that could easily be I saw a van or I saw a van, but its not I saw a van because those words in that combination are very unlikely, whereas those words in that combination have a much higher probability. And pretty much all applications of text analysis involve something like working out the.

Probability.

So the idea is to try to compute for any given doubt, any given sentence or any given text, any given sequence of words, what is the probability of that particular combination of.

Words in that order?

Or another way is to say if you've got words one, 2,3,4, what's the probability of some word being word number 5 predicting the next word? And thats what gpt, general purpose transformer does. It essentially has a model of all the possible word combinations. You give it a piece of text and it predicts the next word and the next word and the next word, and next word until it generates a whole piece of.

Text.

Its generating the text based on its known probabilities and what youve given it before. So this is what a language model is, very strictly speaking. You might call, you might have come across grammar, which is if you're learning English, then you have to learn the grammar of English, which is the rules for putting words together. But in computing in AI, this is typically called a language model. And it's very specifically for computing the probability of every possible candidate given the context. And thats much more than just grammar in some sense. You also have to be able to compute the joint probability. So if you got a sentence like its water is so transparent that what you're interested in is what the probability of that combination, not Individual parts.But you can sort of work Out.

That theres some basic Bayesian probability to help you calculate these things. So the probability of a followed by b followed by c followed by d is the probative a times the probability b given a, times of property c given a, and b times of property d given a, b and c. And you combine probabilities by multiplying them together. And this way of breaking up the probability of the whole chain into little pieces allows you to compute or estimate the properties of the little pieces and then multiply them together. This is anybody thrown by this basic probability stuff. And I don't know, I'm not a statistician, but to me, this looks sort of obvious. And in general, you can generalize it having xs rather than ABS in the CS. So in principle, you could do.

That.

But the problem is when you get to longer sequences, youve got to work out, for example, the probability of the word being transparent, given what weve seen so far, is its water.

Is. So.

And the only way to work at these probabilities is to, in a large corpus, count up how many times these things occur. And you can count up individual words and you can maybe count up pairs of words, but it's very hard to count up how many times does transparent occur after its water is so. Because the sequence its water is so is very rare that any one individual sequence of 4 words is going to be very rare. And so that doesnt really work.

Out.

You can count up the frequency of its water is so transparent that the and divided by the frequency of its water is so transparent.

That.

To estimate the probability that the next.

Word is though.

But that doesnt really work because we cant, we havent got lots of examples of this. Theres too many possible sentences for, to be reasonably representative frequencies of all these things. So what markoff is a Russian mathematician did over 100 years ago now, he worked out that what you can do.

Instead.

Is to say that rather than counting up, rather than saying the probability of the given, its water is so transparent that that's more or less the property of the following, that, so just ignore all the other words, you can work out the probability of a word just by looking at the word before.

And.

Usually that works more.

Or less.

He actually counted it with letters rather than words, but it's the same basic idea. This is this isn't showing properly. It's supposed to be another symbol that it's not only. Never mind. If you look at the textbook, it's got.

The right symbols.

If you work.

Out.

If you want to generate some text, then you can use this marker Assumption to generate pieces of text. The simplest model is the probability of a word takes into account just the word itself. And then if you generate a text, you just take, you generate words at random, but not quite at random, but at random, depending on dependent on the relative probability or frequency of each individual word. And that will generate things like 5th and or futures that are not incorporated. So this is not grammatical, but it does tend to generate more of the common words like the and an and less of the rare words like.

Inflation.

A better way of doing it is to have a biogram model by means 2. For each word you generate, you look at the previous word and for the previous word, you choose the word which is more likely given the previous word. And then that gives you something a bit better like outside new car parking lot of the.

Agreement reach.

Which starts to sound a bit more like English. It's still not very good.

And this works reasonably well if you have all if you have a model based on, if you estimate a word based on the previous 4 words, then you're much more likely to get it right. However, in English and most other languages in fact, pretty much where every other language there are these things called long distance dependencies. Like the computer which I just put in the machine room on the 5th floor crashed. And the dependency is the computer.

Crashed.

If you get as far as 5th floor and you want to predict the next word, the next word depends on computer its not the floor crashed its the computer.

Crashed. Right.

And if you want to see another example if you come to my office brag 2.34 on the door, ive got an example of chat gpt arguing.

With.

A professor of English corpus linguistics about the subject of a sentence in here. The subject of this sentence.

Is.

The computer. No it isn't it's the computer which I just put in the machine room on the 5th floor. Right? So the computer which I just put in the machine room on the 5th floor is the subject and of that, the head of that subject is computer and therefore computer has to coincide.

With craft.

But on the whole these that this is an exception which proves the rule or tests the rule in most cases, theyre not exceptions in most cases you can work with these ngram models.

Okay, how do you estimate these probabilities? Well its in terms of counting the frequencies so a simple example I am sam I am I do not like we get pigs and ham you can calculate the frequency of each word given the context to give you frequencies. So for example.

I.

Appears 3 times I am sam I am I do not like 3 occurrences of I and the progress of I. Given this the beginning of the sentence that thing in angle brackets s things the beginning.

Of a sentence and.

There are 2 cases where I is follow immediately follows beginning of sentence. Therefore the probative I, given that it's the beginning of a sentence, is 2 divided by art 3, which is all the numbers of eyes. So 2, the probative eye being at Venus ends.

Is.

2 times where it is at the beginning of a sentence, divided by 3 times where I occurs anywhere. That's 2 over 3. And you can do the same calculation for all the other bigrams, and that gives you a bigram table. In reality, you want to do on a larger corpus. And heres the Berkeley restaurant project. This is a corpus of people were asking things about restaurants. And here we have out of nearly 10000 sentences, all the bygram probabilities, or sorry, bygram frequencies. So for example, I want occurred 827 times. I appeared 5 times. That seems a bit weird. Why would you say I? Well, maybe somebody hesitated. I don't know. I haven't actually got the corpus. Maybe something that I'm not sure I want to tweet. That is an example. That's fairly rare. Okay, so those are the bigram frequencies. Notice theres a lot of zeros. So I too never occurred. I Chinese never occurred. Some of these have never occurred. You think would be plausible. To want. Sounds visible. The to.

Want.

I can imagine that being a combination. In reality, it just didn't occur in this.

Corpus.

Okay, and then you have to take out the frequencies of the individual words. So I occurred 2533 times, I occurred 5 times. Therefore the probity of I is 5 divided by to 2500, which is a very small fraction. Whereas I want is much more common because it's more likely. Okay, and there's still lots of zeros here. So to want has a probability 0 according to this. And then to work out the property of I want English food, we just take the properties of each of the pairs. Once given I, sorry, I given it to start a sentence, want, give an eye English, give and want food, given English. And in a sentence given food. And multiplying those together, you get point 0,0,0,3, one. And you can do this at home if you want to. And all of these are knowledge about the frequency of pairs of words.

There is a practical problem in computers. A fraction, a probability is a number between Norton one. And therefore, if you combine 2 probabilities, that means multiplying together, and 2 fractions multiply together, will always be a smaller fraction. And you end up with possibility of underflow. You end up with a very small number which then becomes 0, which you dont want. So what you can do instead is you map each of these probabilities into logarithms. And logarithms are these special magic function such that you combine the logarithms by adding them together rather than multiplying them together. And that way you avoid this underflow problem. So a larger logarithm is more likely than a smaller logarithm. And there are software toolkits for doing the sort of stuff. If you want to read.

More about it.

Google very kindly back in 2006, they realized that, okay, people wanting to build language models have to count up frequencies of unigrams, diagrams, trigrams, 4g, 5g. And for 5g in particular, you have to have a very large corpus to get representative frequencies of all the possibilities. And that would take a lot of compute power. So they decided to give to the world their collection of, that they collected, that they processed a trillion words of text and counted up all the 5 word sequences. And there was a billion 5 word sequences and they gave the frequencies of all these and all the forward sequences and so on. And.

There were.

Yes, 13 million in different unique.

Words.

So that's a very large amount of computation. And now people can just upload this and use it themselves. It still builds a very big table just to store it, but at least you haven't got to do the counting. And theres some examples of 4g serve as the incoming, as inserve, as the incoming president, I would guess, and so on. Then, okay, that we have the raw frequencies if we want to.

Do that.

Then another issue if you want to compare 2 language models to see which one is.

Better, which one gives you better results.

And well, you would typically you work out all the frequencies using a training set, which gives you estimates of the probabilities. And then we test the model on a test set. And this is unseen data. And this isn't just about if you're doing language modeling, but if you're doing any sort of machine learning or data lining at all, you have to ideally you have a training set and this should be as big as possible to make it the frequencies as representative as possible and that you have to hold out a separate portion of the data as a test set to, to make sure that you're not including any of the knowledge of the test set in your training set.

Otherwise, if you train on a training set and then test it on the training set, and you can do this in weccer and you can choose train on a training set and then test it on the same set. And that will give you a very good score. But it's an unreasonably high score because obviously it's Learned everything about this training set. And then its come back with the same. Its not perfect because its a biograb estimate, whereas it wont capture long distance dependencies. So you have to have a separate test set and you also have to have some metric for saying how good it is. Well, if in general what you should be doing is if youve got a spelling corrector, then you apply a model on on the spelling corrector and then you apply the other model on the spelling corrector and see which one corrects more spelling mistakes that would make sense or the same thing for speech recognizer or.

Whatever.

But so you just count up how the misspelt words are correctly corrected properly or how many words are corrected properly and you can measure accuracy simply that all the ones it got right divided by all the ones it was trying to analyze, right so if if if it got if it were 15 spending mistakes and it got 10 of them right and it missed the other 5 thats an accuracy of 66%. Thats fairly straightforward. The trouble is that means you actually have to run the spell checker and do all this other extra work. So what you really want to be able to do is work out the intrinsic value of the language model. Rather than saying the language model is how good it works on a machine translation task or for a spell checker or whatever the end result that youll try to put it to. It would be nice to work out how good one language model is just as a language model compared to another language model. And there is a way of doing this, which is quite complicated or perplexed and I havent got time now to describe it. There is some more details in the lecture recording and in the textbook. But the idea is basically you try to work out for given samples of text, how good is it at predicting the next word. And if you've actually got the text, then you do know what the next word is and you sort of forget what the next word is, let the model predict the next word and then compare the modest prediction against what has actually is the next word. So you can do this given a piece of text. And unicam model is pretty bad at doing this because it will just always predict the of or hand, one of the frequent words, which is hardly ever right. Bigram model is generally better. So the idea is to try.

To predict.

Given youve got a test corpus, you go through the corpus, taking out the wolf, ignoring the word, and then using the context you've got before to try to predict that word, and then compare the prediction against what the actual word is. And that lets you compute accuracy of a sort formal language model, regardless of what the actual application is. And just as an example, we see on the Wall Street journal corpus, which was a standard corpus used quite for a long time, you can work out, they worked out the unigrand model had a perplexity of 9,6,2. This is not a probability, it's a score. And the higher the score, the worse it is, the more, the more perplexity it is, and the lower the score, the better it is. So bigram is better than uni gram and trigram is even better than bigram. You might say 4g is going to be even better and 5g is even better. However, there is a.

Problem.

Well, one problem.

Is that.

Let's go through this.

Quickly. Oh, sorry.

That.

If you.

Use 4g, then you, it will learn more and more of the training set, and then it will be very specific to just that training set. So here we have examples of language models used to predict some text. All of them are trained on the complete works of Shakespeare, which is about a million words. And if you just take generate randoms, so random words, where the randomness is biased by the frequency of the word within the corpus, then you'll get to him swallowed confess here, both, which it's just words, and you get more of the frequent words and less of the infrequent words. But they don't make any, make grammatical sense. A bigram model is a bit more grammatical because it makes sure that pairs of words do actually come together. Why does stand forth? Why cannot be pursued? He is this palpable hit. The king.

Henry.

Okay, that starts to sound more Shakespearean. The trigram is even better. Fly and we're with me. These news of price, therefore the sadness of partnering, as they say.

tis done.

OK, that sounds pretty Shakespearean. When it comes to 4g, we get king Henry. What I will go seek? The traitor Gloucester exempt. Some of the watch a great banquet served in. The trouble with a foregram is actually starting to take real bits of Shakespeare plays and just put them together. It's no longer generating new stuff. It's simply repeating bits that you've seen.

And.

If you go to 5g or 6g then you'll end up just generating one of the Shakespeare plays.

Probably.

So there is a problem that many of the engrams which could appear don't appear because the Shakespeare corpus is actually quite small. So for to build these n grams at the level of 4g and 5g like Google did, you have to have a trillion word corpus, not a bit, a million.

Word corpus.

If you have a small corpus and you have long engrams then you will over fit to Shakespeare in this case. And the same thing comes, but this is the Wall Street journal, the same problem.

Appears.

And you can work out from free grams which one. The first one looks like Wall Street journal, the second one is Shakespeare and the 3rd one is probably Shakespeare because its getting bits of text, snippets of text which are from the training set. And this is a problem with chat gpt as well. Its trained on the entire worldwide web but everything it generates is snippets of text its seen before sort of glued together in some way. So it can come up with things which look pretty good.

English.

But actually are false or have some combination of things which put together don't.

Make sense.

But the individual bits do seem to be okay. Now this is the problem with overfitting ngrams work for the word prediction, but in real life it often doesnt work that way. Another problem is the.

Zeros.

So you can get in the training set lots and lots of examples but if a test set contains, for example denied the offer, that sounds perfectly reasonable English, it just didn't happen to occur in the training set. But that means it has appropriate of 0. And therefore if the problem with probabilities is if one, if you multiply them all together, if any one of them is 0 then the whole thing becomes 0.

Probability.

This is an unfortunate consequence. You don't want any of the probabilities to be 0. What you dilly do want is every probability is at least a very small.

Fraction.

So what you.

Can do.

Is because division by 0 is a.

Problem.

You can basically say first of all counts up the actual frequencies and in this small sample we have 3 allegations, 2 reports one claims and one request but no attack man or.

Outcome or whatever.

And then you say, well we're gonna make sure all the words which don't appear will have at least a very small fraction of a probability. Let's give them 1/2 each so attack man and outcome have a frequency of 1/2. Of course it doesn't make sense you can't have a frequency of 1/2 but we're putting in just a just to balance things out. And if you're giving a frequency of 1/2, then you have to take the probability away from somewhere else. Maybe if you want the total frequency to be the same, then you have to make sure you take away half from allegations, reports and claims and so on. And that's a sort of idea about it. That gets to be quite complicated because you've got to go through every word adding 1/2 or taking away 1/2. LA plasse smoothing is even simpler, and that seems to work reasonably well. You basically add one to every frequency.

Count.

So rather than probabilities being the counts divided by the counts, you add one to everything. So here we have, and this then gives you a slightly different maximum likelihood for everything. For example, let's say the word bagel occurs 400 times in the corpus. You imagine it has a frequency of.

401 and right? And that.

400รท1 million is more or less the same as 401รท1 million. So it doesn't make much difference to the words which actually are reasonably frequent, whereas the words which appear 0, if they appear one, then they get a nonzero probability. So that works.

Much better.

So here we have, going back to the restaurant corpus, the frequencies of all the words, whereas there were 5 occurrences of I, now there are 6 occurrences of I. I want is just going up 8-7 to 8:8. So that doesn't make that much difference whereas I too, I Chinese, I food, they all will start to become possible but not very.

Likely.

And then you couldnt calculate again the probabilities and we see all of these, that the ones which had 0 before have a very small probability and the ones which were nonzero have a slightly larger probability. And then you.

Can.

Recalculate all this stuff, you can, you can, this is reconstituting the counts thats given the probabilities, estimate what accounts would have been in the corpus. You dont actually need to do that because you want the sort.

Of probability.

Okay, adding one to every number, that sort of works, its not very ideal because it's not perfect. So there are a couple other ways of doing this. One is called back off. So backhoff says if you have, if youre looking for trigram model, if you actually have that trigram, I forgot was I, if the sequence of 3 words does occur in the training corpus, then use that frequency to estimate the probability. If it doesn't occur, if it's I want to, no, I like to, let's say I like to, as in I like to watch the buckets I to, if that doesn't, if I like to doesn't occur, then you look at the property I like and the property of like 2 and multiply those together to give it a positive I like 2, so use the bigram model instead. If I and like 2, neither of those occur, then you take the positive I, the poverty of, and the probability of 2 and multiply those together. So use the unigram. So that works reasonably well.

Another way of doing this is whats called interpolation. That is, you take the unigram model and the bigram model and the trigram model and combine them all in some sort of way. That works slightly better, but it's a bit more complicated. So to combine them, you have to say, and the probability of a particular sequence of 3 words in the test set, you estimate that by taking the probability, youve got that trigram from the training corpus and by multiplied by some weight lambda one, and then you add on the probability based on the bigram model added, but multiplied by some lambda 2. And then you add on the probability of using unigrammodel, multiply by some lambda 3 and probably lambda one is bigger than lambda 2, which is bigger than lambda 3 because your if you got the trigram probability, its more trustworthy. And that then gives you a simple way of estimating poverties of 3 sequences, even if.

They don't occur.

One more thing is that ive talked about having a training set and a test set. You need this for a for machine.

Learning.

But you ask if, if you what you may do is you try, let's say one classify, let's say the naive base classifier, and it doesnt give you a very good score. So where can you click another one and try the j 48 decision tree classifier and that gives you a better score. If the score is on a test data and you keep on trying, you've trained with the training corpus, you try a number of different classifiers on the test corpus until you find one that gives you the best score, then you could say you stop there. But then you've used the test data in deciding which classify to use. So in some sense you've contaminated your results. You should only be using a test data to evaluate the system whereas what I've just done, what I've just described is using of a test data to decide to choose between the different classifiers. So what you should really do is use the have a separate held out test set or held out data set. Use that to evaluate the model trained on the training data. Once you found the best model and you've decided which one is the best then and only then can you evaluate it on the test data to give you a result which you havent messed about with. Youre not being youre not trying out various different models to see which one is the best. Youve already done that on a separate dataset. So we need to have a training dataset, a held out dataset and a test dataset. And this is true in general of machine learning and deep learning. It doesn't just apply flight to text analytics. 

Another Issue is Out of vocabulary, words which don't occur in the corpus, but we'll see more later about how to deal with that. Finally, Google has got various tricks up its sleeve, particularly because they're dealing with trillions of words and they have a Vocabulary Of billions of words. They've got to do some clever processing and to make sure things are processed efficiently. And if you look at the textbook, it's got some quite clever tricks.

Doing that.

Okay, I think I'll.

Stop there.

Because I realize im going to run out of time if I dont hurry up.

A bit any questions on that bit?

Okay, the next thing was about text.

Classifiers.

And this is basic.

In.

In text analysis most tasks end up as being some sort of classifier. So usually whatever the task is if its spelling correction span detection whatever its basically build a classifier and the task is getting the data into the right format and then build a task so stand detection is out of is it spam or is it not spam authorship detection. Is it James or is it Alexander who is the author in the subject detection? Is it heart medicine or is it lung medicine or what type of medicine is it or in sentiment analysis or customer review analysis, did the customer like it or not like it? Is it positive or negative? And sometimes in sentiment analysis, its just down to a few words that count. If there are positive words, then its positive. If it's negative words, it's negative. Just ignore the rest of the words. So sentiment analysis is very popular because its not just about whether you like the movie, but companies selling things want to know if you like their products or whatever. Politicians want to know if you.

Like them or not.

You might think sentiment is very complex, there's lots of things like emotion and moving things, but typically in computing we're only interested in do they like it or do they hate it? So it's a simple binary choice, although there may be scales, maybe on a scale of 1:5, do you like it? But it's basically.

That.

So it's the detection of.

Attitudes.

So most text analysis tasks boil down to building a classifier. So you have to know something about classifiers. If youre doing the machine learning course, youd probably learn different classifiers. But in essence it's down to given an instance, a document usually, and a fixed set of classes like half feel sad or positive or negative or whatever the classes are. Then you feed this in and you expect for each instance to get a prediction from the set of possible classes. And you can do this for spam detection. You can, if you use Microsoft office, you can set up your own spam filter based on keywords. So that's a sort of rule based system. But typically.

For.

So for AI nowadays we tend to use supervised machine learning. And for this, you have to have a training set. And the training set includes for each document what class it belongs to. And these often have to be labeled by hand. So some person has to go through and say, this customer was pleased and this customer was unpleased or displeased.

And then having got this training set, the model learns to predict, giving a new document what class it belongs to, and you use some sort of classifier. And within weccer, you basically just pick one and then see what the results are. Then pick another one and see what results are. Typically in machine learning, thats how you do it. You try a number of different possible classifiers and maybe try different parameter settings for each classifier. And when you've decided that naive base seems to be doing better than the other ones, then you try different parameter settings of naive bays to see which tweaking does the best. And then you stop because you got.

The best one.

So actually what classify used isn't so important as the rest of it. What's most important is getting the right training set and getting it labeled with the correct.

Categories.

Having said that, I have, the lecture does give some more details of the naive base classifier. Who here is doing the machine learning course? Do you cover naive bays in machine learning? Good. Well, in which case I don't have to go through this. Given that there's no time left, I won't. Let's go through this. But.

Basically.

Its in the lecture. The reason for choosing naive bays is its a nice example for text analysis because it looks, it takes into account the frequency of all of the words whereas a decision tree classifier like j 48 just looks at the most important words. So decision trees are very good for some tasks like stand detection. So im just going to go through this very bright SLI base classifier. Oh spam detection where you just need to look at a few words with like prints from Nigeria say. Thats very good for stand detection. Whereas sentiment analysis where there are a few happy words and a few sad words. Again j 48 might work well for that. Whereas another task like authorship detection therefore there you have to look at all the words. So its not even which classifier is better but it also depends on the task, what sort of task it is. So for certain sort of tasks, naive phase works better. For other sorts of tasks, j 48 works better. So thats why in essence for data mining, you really have to try several of them and just empirically work out which one is best.

Now, the naive base model is also nice to, because it illustrates, is based on counts of word frequencies, is very similar to the n gram model just looked at before. It's essentially an n gram model, but built into a classifier. And you also have laplast smoothing to do this. So you don't just count up the frequency of all words, but you add one to each of the counts to give you a better account. For naive.

Bayes.

From the training corpus, you extract the vocabulary list of all the words thats over unigrams, and then you extract for every pair of words, essentially biograms and you can calculate this. Okay, another thing is if in the test data there are some words you havent seen, then in the unit you remember said before, its a problem. You want to add one to everything because youve got to deal with that. But actually you could just ignore them if it was in the test data because youll try to.

Predict.

The frequent, the probability of the test. And if you havent seen it before, then you don't need to work out how likely it is. You just don't include that in your calculations. And the same thing goes with stop words or high frequency words. So there's a worked example. Again, I won't go through this. If you want to do this, do the maths at home, then have a go at doing.

This yourself. Okay, I'll go for that.

One problem with naive bays and ngram models, theres always exceptions where it doesnt work. I remember I said the computer crashed and thats a problem because computer was at the beginning of the sentence and crash was at the end.

Another problem if in sentiment analysis, if you say, I like the movie or I really don't like the movie. Those are very similar except for the word not. Thats the only difference. If you have negation, thats a real problem for ngram.

Models.

It doesnt. Ngram models takes into account all of the diagrams and most of the diagrams are the same for the positive and for the negative. Negation. One way to handle this is to basically change if you have, if it's not in there, then you label all the words as not. So therefore didn't like this movie becomes didn't not like not this not movie and therefore more the words are negative than positive and it gets a negation, right? So this is just an example that the simple language models work most of the time, but don't work for some special cases. And typically in nlp or text analysis, you have to have extra methods for dealing with the special.

Cases.

And finally, a member, I said that for sentiment analysis, you typically have positive words or negative words and you don't necessarily have to have a training corpus because you can get these positive and negative words from a lexicon. There are various existing dictionaries of happiness words and sad words that you can just download and use those.

Instead.

Okay so naive base, although it's called naive, that says that's the statisticians called it naive, it's actually very good for various tasks. So it's not naive so much as not perfect, lets say its imperfect. Finally.

Oh, I did say.

That no knife base is very similar to unicorn engram.

Language model.

One other thing you have to watch out for is you have to have an accuracy measure or a measure of how good things are. But accuracy isn't necessarily very good depending on the distribution of the data. For example, if you're in charge of delicious pie company and you want to analyze tweets to see how many people like pies, then you want to measure how many people like pies and how many people have not said they like pies. And you want a classifier.

Which predicts.

Whether or not people like pies. Whenever a classifier looking at all the tweets predicts that nobody likes pies, if most of the tweets don't say anything about pies, then it will be correct most of the time. And this is a real problem. You want to measure not just the accuracy, but things like precision and recall, which measure something about how often you are right for a skewed data set. So accuracy doesnt really work. If theres a million tweets and only 100 of them talk about delicious pies, then a predictor which predicts nobody ever likes pies will be 99.99% accurate. It will get the right answer most of the time. Because most of the tweets, they don't like pies because they're not safe. Just not talking about pies. What you really want to do is some metric which captures the 100, or most of the 100. Thats right. So precision is a better way, or recall is a better way of measuring, which captures some.

Of that.

So recall doesnt capture any of those, whereas precision, depending on precision gives you a better score. It gives you, it doesnt give you a perfect score, but it gives you something better than that.

In if you enter any of the competitions, like in caggle or semavau, this could be a possible research project. It could be to enter one of these competitions and typically, rather than saying they have a precision score and a recall score. The problem with that is got 2 scores and they want to have an overall winner. So they use this thing called f score combines precision because 2 times the position and times recall divided by precision plus recall. It's a fairly arbitrary function but it gives you a score which balances precision recall so that everyone can just get one score. And they can get a league table which gives you.

The winner.

And this is very widely used as a way of, as an alternative to accuracy which allows for skewed status problems. Ive already talked about having a development test set. Another thing you could do is multiple cross validation. So in weccer it will do this automatically for you. If you have a training set and you dont have a test.

Set, then.

You can say, lets do, I want to use 90% of a data set for training and the 10% left out for.

Testing on.

But I will do this 10 times over and each time take out a different.

10%.

And that means theres no. 2 experiments where the test set and the training set are the same thing. Each time the test set is different and then the accuracy is actually the average over all 10.

Experiments.

So at no point do you use the hold of a training set to train, but in the end the average is gives you an estimate of the probability if you did have the whole of the training set. And then ideally what you should do is have a separate test set afterwards to measure the actual accuracy of a model against that. Oh and finally if its more than 2 classes then it gets a bit more.

Complicated.

But we'll leave that for now. Oh one more thing. The predictions can be biased by what's in the training set. For example if you have lots of training sets and most of the negative emotions have American names with them, African American names with them, then the naive base model will realize a coincidence or there's a diagram as a pairing up between African American names and negative emotions. And then if you ask a question about negative emotions, it will predict African Americans. So this is in the training set. The only way to avoid this is to make sure the training set is appropriately balanced for your task. And there is an ongoing initiative to try to get hold of balanced, fair nonbuyers training sets. But this is a current research job.

Initiative.

Okay, that's that. Its nearly time to go. But I just wanted to say.

Finally.

To recommend as a parting shock that you try joining some of the social media groups out there. That might be worth doing. So Facebook, there are special interest groups in data mining, text vertex in Facebook if you want to join those or if you want to go work for Facebook research then that would be good idea. I would strongly recommend that you join. LinkedIn is for people who want a job so there's lots of job adverts I get keep getting sent interesting job adverts into other places. You'll get sent job adverts but theres also lots of discussion on data mining and text analytics and chatty TV and so on. So if you want to join then you can go to LinkedIn. Com, search for me and ask me to be your contact and then you can link to me and that will give you over 6000 second degree contacts immediately so that this will worth it.

Core is another one. If anybody can ask a question and the other people in the group will answer your question for you. It's a it's a nice it's a human answer involved in chat. Ttp kailyn. This is another one. This is a knowledge discovery network worth joining. Cargo has got lots of competitions. There's other ones too. If you look at the.

Lectures. Okay, I'll stop there.

Please get together with a team and put your team name next to your name in the spreadsheet. And I'll see.

You next week. Stop your body.

Right. Today, is it coming through on the mic? On the amps now? Good. Thank you. Okay, so I think this is week 3 Of Data binding and text analytics. Have I got that.

Right? Good.

So im going to tell you at the end, but in case I forget, next week is special. So there's actually only 6 weeks of teaching really. So next week, unit 4, there's no new material. I've let the have given a sort of half term for you to get on with doing your coursework. And there's also a test which will be coming. And to find the Test.

You should be able to go to assessment and feedback. Yes, say that. And theres stuff about assessment information, submit my work. If you click on submit my work, then you should see test one and test 2. And if I click on test one, then the questions will start to appear. So I wont show You yet.

But if you try clicking on test one, youll find its conditional availability, which means you can only see it from the 22nd until the 23rd. So it's there for 24 hours. And then you can, its 20 multiple choice questions. For each question, therell be 3,4 or 5 answers and you have to choose for each answer, is it right or not and click on the ones that are right and dont click on the ones that are wrong. Sometimes it will say there are 3 possible answers, in which case you have to click on the right 3. It will tell you how many answers there are, but you have to click on the right 2 or 3 or one to get the Mark. Should only take you a couple of minutes to do this. As long as youve been learning everything, so its based on the materials mainly for weeks one and 2 and also this week, thats all thats covered, the things in the lectures. So as long as youve been watching the video recordings and understanding at all, you should be fine.

Notice also its marked using the leads marking scale, which is 50% or 10 out of 20 is a pass Mark. And then 60% or 12 or 20 gives you a merit. And 14 or above that 70% gives you a distinction. So you do, if you get 14 out of 20, don't feel bad. You've actually got a distinction grade. If you don't, I don't expect anybody to get everything, in fact. And probably the highest score I could remember was 17 out of 20, who joined the class temporarily to find out about the staff. So I'm not expecting you all to do as well as the professor from London university. So don't feel bad if you get some wrong answers. It's not like a math test where it's reasonable to get 100%. Okay, so dont click on that now. Or if you do, it wont work for you. If I go into student preview mode and it will be, it will just tell you that you cant use it until the 22nd. So on the 22nd at 10 o'clock, it becomes live. You can go in and answer the questions. You have an hour. Do not go off for a coffee break halfway through or whatever it is. You want to do all of it in one go. Because if you leave it unattended for 10 minutes or more, then it automatically thinks you've given up and it submits for you, even if you havent finished yet. Sorry, question?

Now you just do test one, you cant read this yet, it says test one, the due date is the 23rd of February, test do the due date is 23rd of March. So test 2 comes near the end of it. Test 2 covers the whole of everything including units 4,5 and 6 and 7, sorry, 5,6 and 7 and then theres a final piece of assessment which isnt doesnt appear up yet, which is the research project with proposal. I'm gonna next week it's for you to get off of your project, but I will electron Friday, I'll go through hopefully an example and some more details about what is im expecting you to do.

But the assessment information, sorry, assessment, click the wrong thing. Assessment information will tell you what the assessment brief is. You can see the assessment brief here or download it yourself. It says you have to get into a group of 4:6 people. If you want to do this on your own, you can do, there is a register, a class list on the class list, I want you to put next to, next to your, let me go back to class list in a minute.

So what you have to do is write a research proposal for a research project which uses some of the data mining and texanalytics tools and resources that were talking, covering on the course. You might want to use sketch engine or chat to TP or wecco or one of these other tools or at least 2 of these other tools in some practical application for example, I'm interested in using it for teaching and learning at least university.

So that's what I'm gonna write about and there is this class list here, which hopefully will open up in another window and in the class list, I see most of you have already started filling us in next to your name, you put next your name and your username and then you should put the name of your team. So there's crazy Thursday and nice team name, why not? And call it wherever you like. We are I quite like where is it? Mining Lab Team One.

Okay, all sorts of force data force well, you can think of a nice name and dont use the name solo is for people who want to do it on their own. So solo means individual alone so there are a couple of people who don't want to work in a team for various reasons if you wanna do it on your own that's fine it means you haven't got anybody else to discuss the topic with so you may find it, its easier to come up with a research idea its a lot easier if you can discuss it with somebody else so when im trying to write up ideas for research projects, thats a good Idea.

You can, if you want, plan this as your MSC project plan, if you like. So then you can go and implement it as part of your mse project. Or it could be for a larger scale project. Maybe if you want to do a PhD after you finished here, then you can develop this as a PhD plan, for example. It's up to you how long you do it, but you have to be, it has to be something with Novelty and so on. Okay. Okay. Well.

Thank you very much for everybody for filling this in. It looks like nobody's messed It up As far as I can see that it all looks quite sensible. If you haven't got a team, then look for the blanks. There are some blanks, and those are people who haven't got a team yet. So you can ask them to join your team.

Maybe. Okay, that's The thing you have to do is to write a research proposal in the style of an epsrc research proposal. And ill tell you a bit more about that next Friday. But first of all, the big challenge is thinking up what is the research topic, what are the aims and objectives that youre going to try and meet. And if once youve done that, then you can fill in the rest of it fairly mechanically almost. Okay, today I want to look at theres been 3 online lectures. Few to watch.

I hope you've already watched them all if you haven't. And this is like a summary of what to expect if you have watched them. This is a summary of the main bits to remind you of them. And this Is The first one was quite a long one. Let's see if I can get This up here.

This is about vector semantics and embeddings. So this is the, if you like, a very central concept in text analytics or in data mining apply to text. If to do data mining or machine learning, you have to have a number of examples or instances. And each instance represents, is represented by a set of Features. And the features in the end have to be numbers because then you could do maths on them. You can measure how similar 2 numbers are by Measuring a Distance, subtracting one from another or if you've got a vector of numbers then you can compute. But how close the 2 vectors are in dimensional space. Straightforwardly with words and sentences it's not so obvious how you do that. What you might think cat and cat are the same word, therefore they're very close together. Cat and cats with nesson are similar because one's got an extra s whereas cat and dog are completely different. And that seems reasonable but then wedding and marriage are completely different, but they're not really. The meanings are similar.

So you want to capture the meaning of a word or a phrase or a sentence by a vector of numbers, such that if 2 words mean the same thing or similar things, they will have similar vectors.

So how do we do this? So embedding is a representation of a word meaning typically in the form of a real valued vector, as a vector of real numbers, possibly the range 0:1 or at least not just integers, its not just one or 0, its not just straightforwardly binary. And encodes the meaning of the word such that words are closer in the vector space are expected to be similar in meaning. And that's Wikipedia's. That's a nice summary.

So this is what we're going to look at. This is quite a long lecture online. It's because it's a difficult concept. I think after this, the other lectures will be shorter, so don't worry about what's coming up next.

Looking at vector semantics, words and vectors, and a cosine metric of similarity. And then a couple of ways, in fact, 3 ways of computing these vectors. One is tfidf or term frequency divided by document frequency. Another ways, point was weekly information, which is used in these neural network models, but I havent got time to go into much detail. And words of EC, which is the current craze to use embedding representations using this neural network representation. So if you want to have a representation of words or the meanings of words, and you've got to have an idea of what is the meaning of a word. So we looked at ngrams or patterns of various sorts, but those just look at the character strings. In philosophy and in computer vision and in other applications, you often have this idea that theres a set of categories and the meaning of something is which category belongs to. If youve got a load of pictures of cats and a load of pictures of dogs, that each of the cat pictures has a label cat and each of the dog pictures has a label dog and therefore the meaning of cat is c a t and the meaning of dog is d o G.

And that also allows you to do some sort of logical inference, like this sort of thing but it's actually not very it's not very helpful as as a sort of not very funny, but standard joke in linguistics what is the meaning of life and the meaning of life is life saying that the meaning of life is life does not add any any useful meaning to the concept of meaning so really its not enough just to say that youve got a variable. Identifier which is the Meaning Ideally you want to have something which is more like human meaning for one thing. In a dictionary a particular word can have several different senses which are different meanings. So you dont have a way of capturing the fact that dear dog doesn't just mean one thing or mouse doesn't just mean one thing. It could mean several things. So this is the idea of a concept and one word can label several different concepts. On the other hand you have can often have 2 words which have the same concept, like couch and sofa mean the same thing. So synonyms are 2 words with one concept. On the other hand many words have got 2 or more concepts. So theres not a straightforward 1:1 mapping between meanings and character strings or words. Furthermore, there's no real thing when I say the same meaning, it's not, there's no real sense where 2 words, 2 different words have exactly the same meaning. For example, you wouldn't, in a guide to surfing, mention h 2 o. You don't need to talk about water because hto doesn't fit there. Or you think a big and large meaning the same thing, but I better not go to my sister, my big sister, and say to her, you're my large sister, aren't you? Because that large system means something completely different from big sister. Big system means older sister, so big has a number of meanings and one of the meanings of big is older rather than bigger or larger in size, whereas large doesn't have that sense. And so this is idea that different forms, different words should have different meanings. And there are some examples from French.

But so the idea of having synonyms doesn't really work. There's no 2 words which are exactly the same in meaning. So what matters more, what's more useful is this idea of similar in meaning. So car and bicycle, they're not synonyms but they do both mean things that we write to work on, let's say. Or cow and horse, they're both animals you might find in a farm. So they're related and you can ask human beings in psychologists have done this to try to work out similarity on a graded scale. In this case nought-10. So vanish and disappear mean almost the same thing, whereas whole and agreement have nothing or very little in common. There are some cases where there is a Similarity.

And you might actually ask people for pairs of words which are related rather than similar. Coffee and tea are similar in one sense because of both things. You might drink coffee and cup. Well cup isnt something you would drink, but you can see that cup often appears in the same sort of context that cup as a coffee would appear in. So they are related or similar are again not quite the same thing, and you can have, you can group together words which are similar in a class or a group or a field or semantic field, surgeon, scalpel, nurse, anesthetic, hospital. They're not synonyms, but they're all to do with industrial disputes, let's say, or hospitals in this Case.

There is, there are the relationships. So meaning isn't just similarity, but also oppositeness. So there are words like dark and light, which are similar in many ways because theyre both about how light or visible things are. But one, there are opposite, if you like, theres several different dimensions of meaning. And on one of the dimensions, theyre opposites, but on the other dimensions, theyre the same in the dimensionality of topic. The topic for both of these is visibility, if you like. So theyre the same on that feature, but on another feature is degree, their opposites. So antonyms form a binary opposition or opposite ends of a scale on one feature or one dimension, but not on all of them. And there are other dimensions of meaning, like effective meaning or sentiment. So words can have positive or negative sentiments, this even though they're not related in other Ways.

So the the, this is quite a few different dimensions. Psychologists have found various things that you can measure, contrast in this valence, arousal, dominance, these are all variations of sentiment. So far we see that there is not just the words are similar or not similar, but there can be similar in different dimensions or different features. You can have similarity relatedness, connotations and so on. So you want to have some sort of representation of a vector where a vector has got, let's say, 100 numbers in it and the first value in this vector relates to the first value in another vector. And if ever same, then on that dimension these words are related. And vector semantics is very useful because if you can represent something as a vector, then you can compare 2 vectors very straightforwardly. So how do you compute what values to put in this vector?

Well, there's an idea from philosophy in linguistics that the meaning of a word is how it is used in the language, or a word is characterized by the company it keeps. So for a particular word to find its meaning, you go to a corpus, you find examples or tokens of that word type and then look at the words which appear before and after. That each of those tokens and the words which appear before and after and the frequencies that they appear before and after That Represents the meaning of that Word.

So if a and b have almost identical environments, then they are synonyms or similar. Right? So a nice example from the textbook is this word on Choi. I didn't know what it meant before I read, but you can see the sorts of words that appear around enchoi are sauta'd with garlic over rice and leaves. And if you look in your corpus, you'll find another word which has similar context is the word spinach or charred stems or collard greens. Those are other words or phrases which appear in similar contexts to enchoi. Therefore, enchoi must mean something similar to spinach or charred stems or collard Greens.

Thats a very simple model. Enchoi is one of these type of things. And if you get some onchoi, you can look at it and you you can see that it does sell. Another thing you can do with is look at a picture of it and see how it is, but were not going to do that. So in AI general you might do that, but as far as text analysis is concerned, you're only looking at a text. So you don't have access to pictures of things. Okay, so you see that meaning is a point in space and there's various dimensions in this space. So you want to be able to see.

That Meaning is some sort of point in multimissional space. And the point in space is captured by looking at in a large corpus for a particular word, lots of examples or tokens of that word and seeing what appears around them. Thats defined by linguistic distribution so each word is effectively a vector not just a character string like good or a, a variable name that w 45.

Now if you've got let's say each word has 100 dimensions capturing its meaning then there are standard mathematical techniques for taking 100 vector dimensions and mapping them down to 2 or squashing them or decomposing them so that you can visualize them. So here we have these are each of these words as a vector of 100 numbers which represents its meaning in a corpus. And then using decomposition thats mapped onto 2 dimensional space and thats done by choosing the 2 dimensions which are the most significant in some sort of way and that then shows you that words like to and by and that are similar, and these are happen to be all words which are function words which appear all over the place, whereas bad and dislike and worst are also similar, but theyre different from to and that and by as good and fantastic and nice as are also similar to each other, but theyre different from good, bad and worse and theyre also different from that. Buying, so on. So that's basically what we do.

Was that question? No, you're just scratching it. Okay, fine.

Okay I said we have this way of taking 100 numbers and mapping them down into 2. So in Wikipedia and embedding is a mathematical embedding from a space with many dimensions per word to a continuous vector space with a much lower.

Dimension.

So you might have, I did say you count up all the words which appear in the context, send the lob corpus of a million words, or there's about 50000 different word types in there, or the complete works of Shakespeare which is also about a million words, there's about 20000 different work types because Shakespeare wasn't quite as prolific as the complete works of the lancasalsabervin corpus. So that means that for to capture the meaning of a word in the lob corpus, there are 50000 other words which might appear in its context. So you have to count up the frequency of each of those 50000 words. That's a vector of 50000. Now for any particular word most of the other words don't appear in its context. Therefore most of those values will be 0, but you still have to count them. Ideally you want to have just 100 numbers because 100 number of vector is much smaller and easier to compete With.

So thats the standaway representation. In most language processing algorithm, chat, gtp for example, that represents a sentence as a sequence of words and each word is represented as a vector of a thousand numbers, or 100. Well I'm not quite sure the number is, but it's probably a thousand numbers. Sorry.

So why vector as well if youre just using character strings like terrible, then you can only match on exactly the same character strings. You cant match with embeddings. If the vector is similar, then you can match all things. For example, wedding and marriage are completely different character strings, but they will have similar vectors because the words that appear around wedding are the same as the words that appear around marriage.

So how do you compute this? Well, going to look at 2 ways. There is another way, but I wont look at that yet. So what traditionally before neural nets came along and people realized how brilliant they were, there was this method called tfidf or term frequency divide inverse, document frequency inverse means divided by, so if you want to basically, if you want to count up, if any particular word, you look at the words which appear in its context, count them up and they have a vector of Numbers.

But counting them up means that words like the and of and which are very frequent, will be unfairly weighted because they're very frequent. So you want to downgrade words like an orvan an which appear all over the place, and upgrade words like zebra and Ong Choi, which are very rare. And that's what the IDF does. The IDF divides by the number of times, number of documents it appears in. We have a Look at a minute.

It doesnt work too well and well see in a minute why other means but lets see how this works. So where to wreck is the modern neural network we're doing it comes up with much smaller vectors of 100 or a thousand numbers, rather than 50000. That's a dense vector there's very few zeros in it. And its done by not by counting up things at all, but rather by training a classifier to predict whether a word is likely to appear. And the neural network has let's say 1000 inputs, you train the neural network to predict yes or no, is it like to be the word and then when it's trained sufficiently, you say the 1000 inputs will take that as all vector representing that word. So it's not that the values on the inputs aren't just straight forward these accounts, but they're done by the neural network training.

Let me just ask in the artificial intelligence course, have you come across neural networks yet? You come or deep learning or one of the learnings? So I haven't got time in this course to cover how these neural networks work. Its rather, its just a black box. But the magic thing about the black box is you have a set, you can choose how many inputs and outputs you have and the number of inputs determines the size of the vector. Okay, so let's start off with a Chinese. How am anybody speak Chinese.

Here?

Anybody want to volunteer to read this? Anybody loudly stuck in.

Here? Go then. Okay, fine. Good for you.

Strong. Thank you very much. Okay. Does that make sense? Is this the correct translation? I should have asked. I can actually speak to Chinese. So the idea is.

That Once you get the fish, you don't need a net anymore. In the same way, words carry the meaning, but once you have a representation of the meaning, then you don't have to worry about the words, the character strings anymore. Okay? So how do you calculate this? Well, one way of doing it is whats called a term document matrix. So these are plays by Shakespeare. Remember, that's the one million words in total. And Shakespeare wrote lots of plays and lots of other things as well. Let me just ask, who's heard of William Shakespeare?

Some of you have. He was an author. He wrote stuff. He's famous in England for writing plays and stuff. So here's some examples as you like it. And 12th night, those are both comedies, whereas Julius Caesar and Henry the 5th, they're about kings and they're about fighting and wars and things. And then for each, each word in the vocabulary, you count up how many times it occurs in each of the plays, each of the documents. So battle occurs only once or 0 times in the comedies, was appears a lot more in the Julius season, Henry 5th, which are about fighting in wars, whereas fool and wit are much more common in the comedies and much less common in the battleWarplays.

You can represent each document as a vector of words, if you like each document. If you did a Google search for, let's say, fool, then you'd want to get all the documents which have got full in them, and the ones that you got full more often are the ones you probably want. So this is what Google search effectively does. It represents each document, each web page as a vector of frequencies of words, more or less. A bit more complicated than that, but thats the essence. And then you can represent the meaning of a document as a vector of numbers.

Right?

But we don't actually. And then you can see 2 documents which are, you can measure how similar the documents are. So Henry the 5th and Julius Caesar have similar vectors, whereas as you like it in 12 night have also got similar vectors. But there's a big difference between Henry the 5th and Julius Caesar on one side, and as you like it in 12, not on the other side. So using these vectors, you can measure similarity between the documents. And it shows that the vectors for the comedies are similar, but the comedies are different from the other ones. Okay, but then that's just for, that's representing, each of the columns represents the meaning of a document. If you look at the rose, you get the meaning of a word. So the word battle appears once in, as you like it, 0 in 12 knight, 7 times in Julius Caesar, and 13 times in Henry the 5th. So this is the meaning of a word, not the meaning of a document. And that way you can represent, you can see that battle and is a word which has something to do with fighting, whereas fool and wit are similar in that theyre more comedic sort of words. And you could do this more generally for other tech samples. So this is from a much larger corpus of modern English and it shows for example that digital has this meaning that in the vector, there's a place for every other word in the vocabulary. So if this was the lop corpus there would be 50000 different numbers representing the word digital and how many times it occurs next to all these other words. And it occurs quite a lot next to computer and quite a lot next to data, but not very often next to pie or sugar. And the same for information whereas cherry occurs not very often next to computer or data but a lot more next to pie and sugar. So we can see that in these terms, the digital information are very close together, very similar, whereas the other words are further apart. Now, I did say close together, and what you do if you map it down onto 2 dimensions and look at this angle, if his angle between the 2 is very small, that means they're very close together. If it's nearer 90 degrees, that means they're far apart.

Tubs don't really want to have a score which goes naught-90. Mathematicians much prefer probabilities which go in the range 0:1. So you want a score which is 0 if they are not related and one if they mean exactly the same thing. And most words are sort of in between, and thats captured by whats called the cosine measure.

And there's A fairly complicated formula for calculating it, which is explained better in the lecture, which I havent got time to go for. But the nice thing is that the cosine goes between one and -1, because we ignore the -1, just say it goes between one and 0. And if you only take positive values. And it means that its a way of computing, calculating from the frequencies the distance between 2 words. So theres 3 words here, cherry, digital and information. And you can calculate the cosine between cherry and information is naught point one seven, thats nearly 0. Whereas the cosine between digital information is point 9,9,6, which is nearly one. So that calculation gives you a score, which is in the range 0:1. And is, it gives you, if 2 words are related in meaning, it gives you a nearly a one, and if theyre not really meaning because youre nearly a 0. And so theres a visualization, if you like, in 2 dimensions, and its called the cosine because that angle, if that mean, if the angle is nearly 90, then the cosine of 90 is 0, which is what you want if they are far apart from each other, you wanted to have a value 0. If they are close together, or the cosine of 0 degrees is one, which means they are exactly the same. And I talked about counting up frequencies, but frequencies doesnt really is too simple because if you just look at the frequency of a word, then you'll find words out there and of and sort of swamp your frequencies.

So what you want to do is divide by, or inverse document frequency means divide by document frequency, divide by the number of documents where it occurs in that and of will occur in all of the documents. So divided by a large number. A word like ardvark may only occur in 2 of the documents to divide by 2. So for example, the frequency of ardvark is 2 and it appears in 2 documents, thats 2รท2 which is a score of one. Whereas the frequency of the, in the whole of a corpus might be 50000. And then if a document contains 50000 documents then thats also the swarf one. But it basically weighs down, it lessens the unfair bias towards the high frequency words.

And so.

Yes, as well as using raw count, this is another thing we saw before, its a good idea to use logarithms rather than probabilities because probabilities combined together get to very small numbers.

The document frequency is how many times, how many different documents it appeared in. For example Romeo appeared in only one document within the Shakespeare corpus whereas action, which has the same overall frequency occurred in 31 documents. Therefore, Romeo is more distinctive. If you thought if you come across Romeo immediately, which claim belongs to, whereas you come across the word action, there's 31 plays it might be in. So that's why you want to count up how many documents it appears in rather than just the overall frequency. And there are some examples of scores. Romeo is a very distinctive word because it only appears in one of these and so on. Okay, so thats you count up the frequencies as a worked example in the lecture, which has got so the. Im going to run out of time if I go through that.

Some other point wise mutual information is a bit more complicated if you want to find out how that works either watch the lecture or read the description. The reason, the only reason for mentioning it is that's what is actually being used by chat gtp. Chat ttp for example does not use tfidf. It uses something like point wise mutual information because generally the results are better. Its more complicated to use and im just going to skip through this very Quickly.

Because I haven't got time. Right. Okay, so word to Beck. The reason tfidf used to be the sort of thing that Google used. You have every word is represented by a large vector of all the other words which might appear in its context, but this is a very long vector for the law of purposes with 50000 numbers, or for the works of Shakespeare's 20000 numbers. And most of these are going to be zeros for any particular word, like falstaff and Romeo. Full staff and Romeo do not appear together. Therefore, the Romeo value for full staff will be 0, and the full star value for Romeo will be 0. So what you really want is a much smaller vector, 50 to a thousand, not long, which is much more dense. So most of the elements have meaning in them, don't have zeros in them. Theyre much better, theyre much quicker, theyre much faster, they take up less space, they also generalize better. The vector which captures the meaning of wedding or marriage will be more similar if it's smaller than if it's very large. So in practice they work better.

So there are various ways of doing this, but one of ways ive already indicated, similar singular value decomposition. This is a way of, if youve got a vector of 50000 numbers, theres a mathematical method for squashing it into 1000 numbers or 2 dimensions. So you can draw a graph of it. The trouble with that is you first of all do the counting to get these large vectors, and then you do this mapping, this transformation or decomposition to get it into small number of vectors. Thats extra computation. You dont want to do that. I mean I just said you want to have something which is small because its fast and compact and stuff. If you do extra computation that means its a waste of time. So that does work but its not very efficient. Much better is to directly compute a thousand numbers without doing counting all 50000/1 and then doing something to it afterwards.

There are other methods, Bert and so on. We'll come up to Bert in week 7 so I'll leave that for now. But wortchevec and glove, these are tools you can use yourself and. I've got about 5 minutes to very briefly summarize how it works. Essentially instead of counting up each word it tries it builds a classifier to predict what the next word is going to be. And the trick with a neural network is any classifier I mean theres lots of classifiers in weccer, youve got j 48 for building a decision tree, youve got naive bays and so on that the difference about a neural network is you have a set number of input nodes and the output node could be yes or no in other words which cause it belongs to and then you go through quite a long training process, which gradually changes all the weights and overnodes and then finally you come up with the optimal classifier which predicts correctly most of the time. And then you can look at the weights on those nodes. And the weights on those nodes, you can take those to be the meaning of that Word.

They don't actually directly count as, they're not the same as the counts of frequencies, but they're just the set of a thousand weights which give you the right word in the training set. And the training set is ready word, the other words it appears in, so that the text that appears into is using the same training data, is just coming up with, rather than counting up all the frequencies, it's coming up with a set of weights which give you the right prediction.

And this is selfsupervision because you dont have to get hold of a training, you dont need a a training set where somebody has marked for each word what the class is. For supervised classification, youve got to have each instance, some person has to add in what the class is. Whereas for this, the information it takes is what words appear around it. And that's already available within the text. So it just takes the text and for every word, it learns from the words which appear around it, which nobody has to add because theyre in there already. So there's no need for human labels. You just take the text and learn for every word what sorts of words appear around it. And there's a skip gram training data method for example if a target is apricot then you can look at the words like tablespoon of jam and and for every occurrence of apricot you look at the words 2 on either side and that gives you then you use that to. Try to predict is it going to be apricot or is it not going to be apricot and if a neural network predicts its apricot most of the time then that though you keep whatever settings there are which predicted that correctly. Okay, so I'm going to have to skip through this to learn this you essentially do I just said you take positive examples of sentences where apricot was in there and those are positive training examples and then you also get some negative examples basically by just randomly replacing or finding some.

Sequences where apricot isnt in there. And in a large corpus thats easy enough to do. So you can find from the corpus positive examples of apricot and the words that appear around it and negative examples of another word and the words appear around it and that gives you positive and negative examples. You use this to train the network until the network seems to predict correctly most of the time. And then at that stage you just look at the input layer and whatever the input layer is thats the meaning of the word Apricot. Intuitively when I first heard this, it sounded crazy. How on earth does that work? Well it just does. As far as up next, maybe next if you go to the deep learning course, you'll find out how but it does. Thats a really amazing thing.

If I just said you look at the word, 2 words on either side. If you do that, then youre more likely to get words which are of the same grammatical category, apricot, apple, pear and so on. If you look at a larger window, then you'll get words which are related. Like you get coffee and drink and cup which arent so closely related but they are come in similar context. And when you use wortivet you can decide what the parameter settings are including what size Window you want. This Has this peculiar sort of algebra feature that.

If you take the vector for apple, let's get some example. There's a better example. If you take the vector for king and you subtract the vector from man and then you add on the vector for woman, then you'll end up with a vector which is very similar to the vector for queen. In other words, king minus man plus woman equals queen or Paris take away France, add Italy you get something like Rome and you can see in general there are the distance between these are pretty much the same, and they are basically the male versus female Equivalent.

And it works most of the time and you can use it for all sorts of other things. For example, if you happen to have a corpus from different times, then you'll find in back in the 19 o zeros, the word gay had a similar meaning to the words like daft or stupid. And then back in the 19 50s, gay had a similar meaning to words like bright and witty and frolicsome. And then nowadays gay means something like homosexual or so that the meanings of words have changed. And you can map, you can capture this meaning in terms of vector space.

There is also a cultural bias as we just saw the male to female thing and if you ask take farther, sorry from the relationship between father and doctor and then what is mother to it will predict that mother is a nurse. So it realizes that the maletofemale relationship also applies to doctor and nurse and thats because in the training set there are lots of biased examples where she is a nurse and he is a doctor so we have lots of examples of that man is a computer programmer as woman, is a homemaker for example so it does depend on what the training data is and this is a current issue in in chat GDP or this sort of research, it will come up with unexpected bias because its not unexpected, its whats in the training data. Okay, I've now almost run out of time, so I've only got very little time left to do the rest of this. There's 2 others, but that was the most important thing that you have to understand embeddings because they are the key to. The key technology for doing data.

Mining on text. So this one, since I havent got any time, I would recommend you just watch the lecture. What this is saying is basically if you have text, then you have the words in the text.

It may also be useful to add some additional information to the text, depending on what the task is. For example, part of speech labeling is adding the label noun or verb or adjective. And this could be useful, for example, in sentiment analysis. You want to look at the adjectives in particular because they are the ones that are useful for determining sentiment.

Another thing might do is name entity recognition. That is labeling the people, the places, the times, the locations, because those are the nuggets of knowledge. And then you have, then you can work out the relationships between them. So these are interesting labeling that you could do on top of just using the text. Since I haven't got time to do that, I'm not going to go there, but read the stuff, look at the lecture Directly.

And then the final thing I wanted to look at Was, where's this One? So I suggested you have a look at some example research papers. And because research papers are quite challenging, I'm not expecting you to learn and memorize everything, but there is another reason. So there are lots of research papers out there you can get at. You can use Google scholar to find research papers for data mining and text metrics.

You might look at the association for computational linguistics. They have an ACL anthology webpage where they've collected together all the papers on computational linguistics. And it's available there as a corpus for you to do sketch engine stuff on if you want to and but a reason for looking at the search paper is because they are structured very much like the coursework that you have to do. So I'm asking you to come up with a research proposal a research paper is pretty much a research proposal plus the results of actually doing the research so the one thing that's extra in the research paper is usually some results and Conclusions.

But most research papers do start off with what are the research aims, what are the objectives of the research and then theres usually some background and what other work has been done, which is related to what you're doing and then you have a segment of the methods, what is the programmer of research that you're actually going to do, and then theres some results but im not expecting you to come up with any results because its not so part of the plan, its what you come up if you actually implement it. And at the end is usually some sort of conclusions, what is the major contribution to knowledge, what is the importance and for your proposal you have to say something about what you expect the results to be and what given, if you succeed why will it be important. So the proposal can sort of be a pilot study for this sort of thing.

Okay, and quickly some examples I wanted to include an interesting paper that I wrote because im very keen to promote my own work, and by all means site my paper if you want to, that's good. Okay, so this was a very, quite a long time ago, back in 1986, very early days. Just to show you how difficult it was to do any computing in those days, the main university, main research computer, icl 2960, it ran for about 5 weeks to do this and in those 5 weeks it was able to process a corpus of 200000 words, a huge corpus of that time, but only the words which were very frequent, so only 175 words in that occurred enough times and what it tried to do is basically.

Oh, hang on.

Hang on. It's taken out the.

Pay the cap one can I?

Sorry. Which for some reason it's hidden. Okay, this, you probably can't see this, but what it shows is pairs of words which are similar by computing for every word this large vector of all the other words, which appear in its context. So the meaning of the word was all the other words and their frequencies. And 2 words were similar if they had similar vectors. And this only really works if you've got a high, a lot of examples. So it turns out, for example, in and 4 are similar, is and was are similar because the words appear around is and was a similar. And most of these words are grammatical words. It worked out for example, world and country are similar because the words that appear around world are also the same are similar to the words that appear around.

Country. So that's Probably more interesting Michelle banco, Eric brill at Microsoft research because they've got lots of compute power and they were able to come up with much better results. And what they did is they tried expected comparing a number of different machine learning classifiers so that each of a different lines is a different machine learning classifier. And they want to see what happens not if you choose which classifier is the best but rather what happens if you give it more and more data and they find that no matter what the classifier is if you give it more training data it will improve inaccuracy. So thats why the scales go up and furthermore that the brown one which was the most the best one with a small amount of data if you give it more and more data it gets better and better but its worse than all the others if you give it a. Large amount of data. In other words if youre comparing classifiers don't just assume that the best classifier for your training set is the best. It could be that your training set is too small if you give it more data you get a better classifier because in the best classifier it doesnt matter which algorithm you use but rather how much training data you have on the however if you give it more. This is training data as against processor power required so if you give a lot of if you get a much larger amount of data, it takes more processing it takes more memory so it's not free to just use more data.

Another thing they found is if you have an ensemble basically a majority vote if youve got 4 classifiers you see what they will predict. If 3 of them predict a and one of them predicts b, then the ensemble will predict a, an ensemble generally has a better score than any one individual system. And they showed that here too, except when you get to very large amounts of data, then voting isn't such great. But generally, if you got anything other than huge amounts of data, then an ensemble voting system is better than anyone individual classified.

They did also point out the trouble of getting large amounts of data is it can be expensive and processor intensive, and so on. Therefore, they have these ideas of active learning and semi supervised learning as better ways to get more data with about too much extra effort.

Now finally, they said efforts towards increasing the size of annotated training debt collections is more important than comparing different learning techniques on a small training corpus. If you in your machine learning course or deep learning course, if all you ever do is try out different classifiers on a small training set, then you're never going to get a very good score. The way to get really good scores is to get a lot more data.

Finally, this is paper on the word of x system. And it's got again, this is like the style of a research proposal. It's got the aims and to come up with a new model architecture for computing continuous vector representations from very large datasets. Thats what the aim was, the background is that there are vector representations and various other neural network models that don't work very well because they take a lot of processor power. So he came up with a much faster way which takes less processor power. The methods are this continuous skip gram method. And if you read the paper, explains that in better. And then conclusions are the results are that his method works much faster than the other neural network models. And its also much faster than the traditional tfidf model. So its generally better. And there's some pretty pictures and there's some more stuff right now. There's some examples of some of the results. And ive been told that its no longer available in Google code repository, but if.

You Google Word to that you will find it elsewhere. Word of it is out there. Okay, I think ill stop there. I invite you to have a look at papers if you haven't yet. So next week there's no lecture videos for you to watch. You just get on with your you get together with your group try to figure out what your research project is going to be. Do The test.

Do not forget to do the test because its 20% of your grade is very easy. It should only take a couple of minutes. Don't go away and have a cup of tea. Make a tea before you start the test and then power your way through a test in 5 minutes. So how long it takes? And dont forget to press the submit button at the end otherwise you wont get them up.

Any questions? You can put them on the discussion forum. Oh, sorry. One more thing. Sorry people. At least one person has asked about the labs and for this module there are no, the only assessments are these tests and the final project so that there's nobody actually at the labs to ban them. Because there's no exercises for you to do the activities in weeks one and 2 optionals. Just so you get a feel for what's available in sketch engine in a weccer so that you can include them in your research proposal, if you see fit. Okay, thank you. You can go now. Have a nice weekend and enjoy next week. I want to make sure that.

And careful, can you hear me okay? Were recording. Thank you for coming today. Still back down to here again and today is week 4 and as youll see, theres no actual new material coming up this week. Am I allowed enough you to hear me? If you hear me okay, I'm getting some echo here. Maybe I'm too loud. Right, so im not this week its basically a catchup week.

There was the test and im very pleased to say everybody has done the test. Im going to ive had to go and check with some important people before I actually release the tech marks. I think im trying to release the marks this afternoon so you can see them as might be expected. In fact, ive been asked by our director of learning and teaching to ensure that the grade profile is appropriate.

In other words, theres unfortunately a few people who failed the test. Therell be a few people whove got distinction grades, but the vast majority of people have got either a pass or a merit, and that is the correct distribution. It doesnt want to have too many people failing or too many people gain first or distinctions. So don't worry, if your Mark is just 14, that means you've got 6 answers wrong, you've got a distinction. So 4th, a pass Mark is 50% or 10 out of 20. A merit Mark is 12 or 13, and 14 and above is a distinction. So im not expecting and pretty sure nobody got you're not supposed to get.

Furthermore, don't worry, don't fret about what the answers were. I would just say forget about it now and get on with the rest of the course. And make sure you know, because the main thing is this project report. So today I want to talk a bit about more about the project that you're doing, to give you an example of a possible research project outline and how to go about writing it, because thats going to be 60% of your grade. So thats more important than this little test that just went past and cover.

Let me just, oh, one other thing of course is I'm here to answer any questions and where's the cameras up there somewhere? Those who are watching this online you can't ask any questions cause it's online to afterwards but you can post questions to the discussion forum. As far as I can see almost all the posts of discussion forum so far have been who I want to find a member for my team. Can you join my team? Ive just looked through the spreadsheet and everybody here now has a team according to this. Theres nobody who hasnt who doesnt have a team name next to their name. That's good and some interesting time names, ultra men. Do we have any ultra men here? Well anyway, that's quite far. The undergraduates have got come up with some even worst names for their teams. So these are all fairly plausible and reasonable, I think. Okay so let's just remind ourselves what it is you have to do.

Here is the assessment specification. Lets see if I can make it a bit bigger so you can actually see it. Make this look because you can actually see it. Its going to work. Okay, well youve hopefully read this before as a test. Youve just done another test. Coming up in week 8, im teaching this in a more compressed mode. The idea is that teaching everything in the first 7 weeks of term and then weeks 8 will be the final test.

Week 9 is still teaching but not for this module. But then you have lots of assessments and courseworks for your other modules so you're free to get on with everything else.

And then week 10 and 11 is after Easter and as far as I can see it's crazy to do some more. Have a break of 4 weeks and then try to remember what you're learning. So I'm not doing any more teaching then and you just have to submit your final assessment.

Week 10 which is when you get back from the vacation. I strongly recommend you try to finish this before the vacation because some of you may be going away on holiday or something over Easter. I can't because I'm teaching in southwest jiaotong, China but some of you may in which case it's hard for a team to work together if some of you are here. But try to get this done more or less before the end of term. You got as far as this module is concerned, week 9 and 8 are free for you to get over that. It is a 12 page report. Thats one report for the whole group. So maybe on average each of hes going to be writing 2 or 3 pages or less. And its not just 12 pages at random but theres very specific sections you have to include and most of these sections will be about page long except for 2 which are more important and can be up to 3 pages long. So you have to write something about the background to the research project, what is the contribution to knowledge, why is it important, who are the users of what youve come up with, what is the research hypothesis and objective, what are you actually going to do? Well, sorry, what are you aiming to do, what are the objectives? And then the program of methodology thats more, thats longer, how are you going to do it? And you have to think of way your front research could be anything you want as long as you're making use of some of your technologies covered in the course so you have to use something like sketch engine or weccer or chat ttp or chatbots or one of these other. Technologies mentioned on the course. In fact 2 of those preferably at least 2. And that does constrain what you're actually going to do.

And then there is also a work plan diagram which diagrammatic captures that. And then final threepage appendix. And the appendix is if you're going to use things like chat gtp or Google scholar or Microsoft word in creating your project report, then you have to explain how you use these technologies. So what prompts did you give to Google scholar or chat ttp. And first time it didn't work properly and then you have to refine your prompt to get something more. So you can use these to help you create these reports, but you have to explain how used it. And thats legitimate on this course because youre supposed to be learning about how to use these technologies.

So I'm going to ask you and there is a big meeting coming up organized by university next month for all academics on whether or not we should be allowing students to use chat gtp. And I will be arguing to say yes, it's perfectly okay. There will be other people saying no, we must ban people from mustn't let people use calculators or computers or anything else and just go back to having exams in examples and right by hand luckily your exempt for now maybe next year students will have to go back to not using computers. Maybe okay any oh that the marking scheme will reflect this so theres the epsrc when I when I submit a proposal to the engineer, with physical science research council they give it a grading of 0:6.

Therell be a Mark of 0:6, except for the big 2 for the program methodology, which can be up to 3 pages long, and the appendix describing how you're using these tools, that's another 3 pages on. So they definitely get 0 to 6x3 or 0:18, and that adds up to 60 marks in total, which is the percent that the grade that you've got to go towards your final degree class. Another thing somebody is asked about referencing the university recommends you use this tool called endnote. Ednote is a rather complicated piece of software for building up a database of everything you read. And in the database, you have to enter for each paper the authors, and usually theres more than one author, and the date it was published, and the name of the paper and the journal it appeared in, and the page numbers and the volume number or the conference proceedings appeared in, and who is the editor of the conference proceedings, and the location of the conference and all this stuff. It takes forever, so you can use endnote if you want to. And then once youve entered the audience of a database, and you can select what format you want the output to be, and it can maybe in Harvard style or whatever else you want, and that's if you want to do it that way, that's fine. It's a lot of work for one report.

It's worthwhile if you're going to be using the same database repeatedly for other purposes. Say for example if you want write your project as something you might do over the summer and that basically becomes your mse project, then it might be useful to keep this to use in your MSC project report. Or if you come back and do a PhD, then you can use this database again for this purpose. Its like creating a big database of papers so that you can extract and generate a few references is a lot of work. You may as well just type the references directly into the word document as easier for the one off shop. And there's a link to Adam kilgaref is a friend who advocated a much simpler style that you basically just put down the name of the first author and the year and the name of the paper and if you can find it on Google by that information, then that should be sufficient. You dont really need to have the rest of the stuff remembering that you've only got a limited number of pages. So don't you need to dont put a lot of detail into the references because thats waste of paper. Youre much better off spending the paper on describing in much more detail the programmer of research and the other bits.

Any questions on any? Of this? Has come towards there is a page count the appendix where you explain how you use Google scholar or chatty fee. That's 3 pages. There isnt a separate section for references so you got to fit them in somehow. So there's a total page limit of 12 pages and I don't I'm not saying how you split these up. Im just recommending that you probably want to spend about a page on most of the sections except. And the marking scheme gives 3 times the marks for programmer methodology and 4 appendix. So it makes sense. I'm expecting 3 times as much content for those 2 things, but there isn't a part of the marking scheme for references. I'm not going to be marking the references. So it's a waste of your time to spend a lot of time on the references. Except that actually citing references is good. It convinces me that you're talking about the background section.

Oh come on. Let me go straight now to, lets go ahead, come out here. And actually in here I've uploaded a PowerPoint presentation for the online masters course. I actually recorded a lecture on how to go about doing this sort of exercise because theyve had to do it as well. So what I thought ID do is ID go through this now, which has got some more details.

Okay, let's see if this works. Okay, so this has got some more information and they'll go future receptions and then at the end, I'm going to go back to the beginning and go future receptions for a specific example.

Do you give me some idea about how to go about doing this? Okay, so the in the UK, the engineering and physical science research council fund a lot of research in universities in AI and but also more generally in engineering and physical sciences. It used to be the European union funded a lot of research in UK university. So I used to have ive had a couple of European union research grants, but that stopped now because the government decided to stop taking their money or rather stop universities taking their money so we havent got any more money we did have an edgbots research project looking into chatbots so youll find out about this next week, I think is a week after okay, so if youre proposing to them theyre even more strict they only let you have 7 pages and 6 pages for the research in its context that is the the background the contribution to knowledge the importance the research hypothesis and objectives and the programming methodology so that only leaves about. One page each you can also write have an extra page to put a diagram of some sort, explaining the work plan, something like a Gantt chart. You dont have to use clever software to write a Gantt chart. You can just, in Microsoft word, draw a table and have a row for each piece of work or work package or whatever you want to call it. And the columns denoting the days or the months or the time periods. And then in each cell, you say what you're going to do in that particular work package for that time. So you don't have to use complicated software.

But the most important things, this is the order thats supposed to come in. So you have to have the background first and then the contribution to knowledge and the importance. But actually the most important thing in to start off when youre thinking about what research book is is what is the what are the objectives what are you trying to prove and for this you need to explain what's new. Well if it's research then you must be doing something that hasn't been done before otherwise it's not really research but it does not be completely new it might be that your you've got a new dataset from somewhere and you're going to try existing classifiers or machine learning or something. On the new dataset and the fact it's a new dataset makes it new or alternatively maybe there's an existing dataset that other people have tried working with and youre going to try something new some new classifier on that data set its whatever the combination of things youre doing has, to be a combination that hasnt been tried before even if some of the parts of it are not new.

Ideally you want to have some measurable objectives so you want to say for example im going to try out different classifiers and the existing classifiers have an accuracy of 62% so my aim is to come up with something which is better than the existing ones and then you can. Try it out if it turns out well since you're not actually doing the experiments you're just posing what you're going to do then that's the safe things to have as an objective. It may turn out that whatever you try doesn't get get heads up in 62%.

But that's the problem of the researchers later on. You're just proposing something for now. So you have to think about what measures youre going to use for saying you have succeeded. It was after the state the deliverables that this is a European English word for European research proposals, something youre going to deliver. So youre going to deliver some new method or some new algorithm. Maybe its a bit hard but or maybe youre going to collect a new data set that hasnt been available before. Or maybe youre going to build some new software of some sort or maybe even youre going to build a website or a new web service using some existing algorithm but its now out there.

For example, chat. Gtp, the gpt has been around for a few years but only academics were using it. But once they put it online as a web service, suddenly everybody is using it. So this is a way of increasing, making your whatever is youre doing part of the search can be how do you, if you make it a web service, is it going to be more easy to use? So having got the idea of what you want to do, then you have to think about how are you going to do it and this is the 3 pages or 18 marks out of 60s. It's well worth getting this right describing a program research steps that you're going to do a b c d for this you might want to consider the cross industry standard process for data mining christgm this is a number of steps in any data mining project so you might say, youve got 3 work packages and for each of these 3 work packages youre going to go through first of all identifying the business objectives. What is you actually need to do and then identifying the data that you're going to analyze and then transforming the data into the right format so that then you can do the modeling that building a classifier or whatever is you going to do it, analysis and then you're gonna evaluate the results and if they're not good enough, then go back and do the thing again until it is right. And then once it evaluates you then you have to deploy it, put it, don't just stop there but put it to some practical use.

Thats the phases of Chris gem. You might do that for the whole project or you might do that for individual parts of the project. And if you've got several work packages and each of these is divided up into 6 phases and you can start to see how you might fill the cells in a table which is the work plan and the one page work diagram should match what youre describing in the text, but in a visually more appealing way if you like.

Okay, having decided what youre going to aim to do and how youre going to do it well, if you like reverse engineering, how youre going to do it well, what youre going to do has to be something novel. Therefore, to prove it is novel you have to look at fine literature or research papers about other related stuff that has already been done, so that you can demonstrate what you're doing is similar but different to what they've done, right? And if it's similar that's a good thing because then you can reuse some of their methods. If they used this svm classifier and that turned out to be the best classifier in their experiments, then you should consider using svm as well for your experiments, which are similar. And so you want to have other related papers thats the academic context. If you can, you might want to, I mean, epsrc, because theyre funded by the government, were not just interested in academic papers, but also policy or societal references. So if you find that youre doing something, I mean, for example, there's a lot of work you've done on English and you want to do it for the  out of my head, for Spain hasn't got the same sort of resources, therefore gonna do this for Spanish rather than for English, and therefore that has societal impact in Spain. Or if youre doing this for Urdu and Pakistan, there's not many resources for Urdu as compared to English, and therefore it will have, whatever you do will have impact in Pakistan. Those are potential beneficiaries.

So you should remember to site references, but remember you have to fit these at the, probably want to fit these at the end of your document, even though you cite them in various other places without the document. So you dont have a separate reference section for each of the different sections, you want to put them together at the end and as the contribution to knowledge.

So how is it really useful and how is it beneficial? So its novel but also, and it can be a contribution to computer science or artificial intelligence. But if you're, for example, doing something on AI for education, and it might also be useful for education. So its other people who benefit from this. And theres also importance, which is how is it going to make the UK or maybe your home country, wherever your home country is, how is it going to make that a better place? So it might be if youre developing AI for education, then and youre from Hong Kong and this will help educate people in Hong Kong and help the Hong Kong education system. And there will be economic benefits too, because therell be companies who reuse your software and sell it to schools and universities.

Epsrc say you have to see how it relates to the research areas and strategies in epsilc. Im not too bothered about that. If you go to the psrc website, you'll see they've got various important things like sustainability, and they want to make sure that the world survives, things like that. I dont quite see how data mining and text analytics directly applies to sustainability, but if your project does, thats even better.

As something else, people say you have to think about the duration and manpower of the project so ive left this open to you might if youre embarking on an MSC project from now until the end of summer then maybe you can plan for this this could be your plan for your mse. Project I mean I think you may already be starting so this might not be the right best way of doing this but if you if you whatever your project is if youre thinking of using data mining text metrics here is the opportunity to plan in some detailed project or? In terms of if youre thinking of doing a PhD afterwards then you can come up with a draft of a PhD plan and then if when you apply for student ships, you send this plan in as well and that will impress whoever is youre trying to get the studentship off, because theyll think youre not just a pretty face, but youre actually, youve got a plan for your phds. Okay, but you can, if you want to make it bigger, you can have it for 2 or more researchers for as many years as you want, but make sure that you can justify or if youre going to say you need 3 people over 3 years, and your work plan has to be very difficult and very time consuming and you should be able to identify the contribution of each member of the research team. So if youve got a big team that makes a bit harder for you to sort this out.

Okay, the big challenge is what project should I do? Whats the research to do? How do I think of an interesting research idea? And this is one reason for being in a team or group that you couldnt maybe bounce some ideas around between you until you come up with something as a TV program called the apprentice anybody see the apprentice? There's one person. OK, so it's a UK TV show and the in the apprentice they get people in 2 teams to do silly things of various sort. No, to come up with interesting novel ideas of some sort and they always start off by getting around the table and discussing what theyre going to do. So thats what you could do. Its a brainstorm but a number of possible sources. One is if you've read some of these research papers. I mean on the course I've recommended some research papers. Most of them are pure research but maybe you can think of applying it to your own, either if you work your work that you have or a company that youre involved in or possibly a hobby that you have. If youre dead keen on gardening maybe you could apply data mining to gardening in some way.

Another possible idea is there's lots of research done on English. Some of these papers I've asked you to read are on English language chat. Gtp is originally for English and so your project could be to see how can you adapt it to another language, you choose the language and what are the challenges of doing that. Another thing you can do is theres lots of existing data sets. If you decide to collect and label a specific new data set and then do some machine learning experiments on that, could be worth doing. Or you could look at various competitions around like caggle and semavalve. These are websites which organizations which run every year of competition off more than once a year. And you can look at there, for example, semeval, that every year theres about 10 different competitions. You could look at the semival 2023 website or the semavalve 22 website, see if any of those competitions look interesting. And then your project could be to look at the existing competitors and see if you can come up with something even better than they did. For example, we might get together their solutions and come up with an ensemble. So semavalve, there was an offensive language identification, or olid task. Google gave them thousands of tweets, and each tweet was labeled, was whether offensive or not. And if it was offensive, was it a personal slur or just a general slur, various other subclasses of offensiveness. And they did this for English but also for Arabic and Hindi and several other languages so you could you could you could do that. Task, but that could be your research project is to come up with a novel better way of doing that.

Another source of ideas is to take a recent PhD project so you can, on my website ive got lists of past students ive supervised in the area of data binding and text analytics, or you can just if you just Google leads university PhD, youll find that theres a an archive of past PhD thesis you can scan down and find a title that looks interesting and then do something like that, but a variation of it, for example, do it for Hindi rather than for English. That might be a simple thing to do. You dont actually have to do it. You just have to write a plan for how to do it. Typically you have to remember to use at least 2 of these methods or techniques or resources introduced on this module. Apply to your own company or language or task, whatever that is. Okay. Yes.

Before I summarize, lets go back to the beginning and give you an example. So I was going to say an example research project I've been toying with. You wont want to take this up as one. This is your proposal.

Wouldn't it be interesting to use chat ttp for teaching and learning and assessment, at least university. As I've just said, there's a big meeting coming up, so im recommending that you use chat ttp and other techniques to help you draft a really good research proposal, so long as you add an appendix at the end explaining what it is you did. But maybe we need, I mean, if you go to LinkedIn or various other social media, there's full of posts about people having ideas about how to use it. But rather than just do these off the cuff, one off ideas, why not have a proper research project on the use of chat ttp and text analytics in university education? And you could imagine doing it several different ways. So the overall hypothesis is to investigate ways of improving university education using text analytics. And thats fairly general, so we have to be a bit more specific than that. We have to pin down more specific objectives.

But what we meant, what do I mean by better, was these 3 things I can think of. One is more effective teaching and learning. So using these tools to help teach students. Another is more efficient assessments. So you spend a lot of time doing the coursework and lectures, spend a long time marking the coursework and marking the exams. Maybe we could use text analytics to automate that. So that's the second strand, that's efficiency, make it more efficient. And the final is to make it more to work.

The university is worried about academic integrity. There was students cheating by using chat ttp to use their course reform. So we need to have a measure of some way of chatty tea proofing all of our assessments. We have to. And so we need the 3rd objective is to come up with a way of deciding or measuring an assessment exercise to see is it cheatable or not. And that also involves using text metrics.

So we've got 3 more specific objectives that we want to have and deliverables are going to have. Well, first of all, some general understanding of this topic area were going to write some research papers, thats always a good deliverable for research project. Youre going to have a data set. So the data set is im going to collect together all the assessment specifications across these university and in general for all of those, thats going to be a data set. Im going to annotate these with and how efficient and how effective and how integritous they are. Ill come to the programming in it to explain that better. And the software were going to have where were going to have a tool within Minerva so that electrode can click a button given any assessment and it will assess whether or not it's a suitable assessment according to these criteria. So these are tangible deliverables at the end. So the program, what are the steps in this research?

Ive got these objectives lined up. I want to measure these 3 things to do with teaching and assessment. The first task then is to collect a data set of teaching assessment initially for the school of computing. And then later on for at least one school in ilk to the faculty says about 7 different faculties that be psychology, in medicine or English in arseniities or geography in the earth and social science faculty, I think. And then later on there'll be the whole of university as a sort of 3rd stage. And the data I'm going to collect is the from Minerva the coursework specifications and also some metadata about the core threat certifications. Metadata will be how long did it take the lecture to write this and how long did it take to market all and how many students were there and how many credits did they get. That way you can measure the efficiency of a piece of coursework at least for the lecture in terms of how much time it took, divided by how many students and credits it was worth. And that for example if if there's 5 students and there's a piece of coursework which gives them each 10 credits and that's 50 student credits and it took me 10 hours to devise and Mark this, that's 10รท50 which is 1/5 of a student credit, whatever. It gives you a measure and then you can compare different ones.

Okay, so thats one of the things im going to do is collect the data, come up with some annotations and then come up with some metrics, some measures for how good things are and the workflow. And I can put a time line for this. Im going to say spend the first 2 months collecting this from the school of computing, and then the next 2 months collecting it from 4 or 5 other schools. And in the next 2 months im going to try to develop some software which will allow this to be collected automatically from via Minerva. So im not going to try and collect for every model across the university, but rather im going to try to automate the process. So that's another thing you might want to consider is have a prototype first where you collect things by hand, a small sample of data and then the next prototype is to automate the data collection and annotation process so you can do a much larger experiment.

Ive mentioned the crispian methodology, so I would want to break it down into the crispim phases. And the diagram you can see I've got these 3 objectives and I've got for each objective 3 levels of prototyping. First of the school, then for the for 4 other departments and then for the whole university so thats 3x3 or 9 things and for each of those, ive got the Chris vm phases so already its starting to look like a quite detailed sort of table of things and it's a very well defined work program.

Having got in my head what im going to do and how im going to do it. I have to justify that it hasnt been done before so I'd use Google scholar, I'd search for other related research on chat ttp for education and it turns out there already are. I did this before coming to the lecture today. There already are for example there is a paper out there on using chat ttp for teaching nurses and universities nursing education. We have a school of health care here at Leeds where they teach nurses, and you can get a degree in nursing, and to be a good nurse, you have to have a lot of interpersonal communication skills. So thats part of the things they teach. And theyve already started to develop tech using tech ttp for generating appropriate conversations in doctor patient or nurse patient communication, that sort of thing.

So there are these papers, but I've noticed so far nobody's done it for computer science. So I found other related papers, but I found a niche that hasn't been done, and I'm going to fill that niche. So I'm going to look at computer science and then im going to look at how it generalizes across all disciplines in university, rather than just one or 2. And this does relate to other work in using chatbots. So there is a, it's a paper by abu shawar and atwell from 2007, I think it is, on chatbots, are they really useful? And that describes various practical applications of chatbots, including teaching but only teaching languages. Theres another example of their related work, but that they doesnt do exactly what im going to do, but therefore what im doing is worthwhile.

I have to cite these references so that you sort of stuck between 2 opposing forces, you want to cite as many references as possible to show that you really understand the area on the amount. Each reference takes up some space, so you havent got, you got a limited space, so you have to have a limited number of references. The best sort of references if you can are survey papers because survey papers capture lots and lots of research, right? If you can cite a survey paper which describes all sorts of research in a field, thats just one citation, but you can mention some of the things that it covers without having to cite all the individual papers that its covering. Contribution to knowledge, well, at the moment is it, nobody knows how to do this so it will be very useful to AI research to know how to use AI systems like chat, ttp in teaching. It will also be useful for the school for the discipline of education. They will learn how to use AI tools for education. It will also be useful for nursing and fit every other discipline within the university. So thats a very general thing.

Wide applications. Its also important to UK plc because therell be companies who are interested in using this technology and selling it not to UK universities, but also abroad. So once we've developed this system here at least university then were probably partner with some company who will sell it around the world, because lees isnt very good at selling things. So currently we have an online master's program partnered with Pearson. So for every student fee, Pearson takes half the money because theyre good and they sell it abroad. Whereas they recruit staff as students onto our course and we get to keep half of the money as well for teaching the students. So in the same way we partner with some companies to make money out of this.

The duration I hadn't really specified but it's starting to look very complicated because there's at least 3 strands of research and there's 3 sets of prototypes. So I'd want at least 3 people over 3 years.

So probably this would be at minimum 3 PhD projects or 3 research fellow projects over 3 years. So if youre going to do something like this, you might want to cut it down to something manageable by one MSC student over the summer. And that means you got to be more specific about what exactly youre going to do. Okay, that's in very brief an outline of a research project that you might want to take this and run with it and adapt it yourself. Or you can do something completely different.

That's the summary again. I was actually going to stop early because I was going to invite anybody who hasnt got a team to stay behind and everybody else can go home early and then the ones who stay behind can form a team. However, that didn't work because you've already all formed teams. So that's not necessary. So now you can either just go home early or you can use this opportunity to ask me anything you like. Question?

Okay, for those of you at home who didnt hear because he hasnt got a microphone, she was asking about wecker and labs. So yes, theres a sort of misunderstanding about these labs. The labs are there for the wecker exercises. There is a wecco tutorial online that you can go through. Its very detailed. If you basically just follow the instructions, it should all work. And if you really want to, I can put on the webpage the answers now because youve had all that chance to do it, but you really dont need to know the answers. The reason I'm asking you to use wecker is the sort of a meta level. That wecker is an example of a toolkit where you can do machine learning without having to do programming. And you should all have some experience of programming and the contrast with a toolkit and see what the differences are. The specific minutiae of how to solve those exercises aren't really that important. Furthermore, the labs are there for those people who don't want to stay at home, but you can do this anywhere. And actually, we haven't got any lab assistants who know the answers is the real answer, so I've been assigned 3 PhD students as lab assistants and having talked with them, they don't really know much about weccer, so they wouldn't be able to help you anyway. They're really going to be marking your coursework in the end. That's what I'm gonna be using them for.

Okay, I mean if you have a specific question, you could always post it to the discussion forum enough and try to answer for it. Really, I mean my attitude to learning pieces of software is I don't like to learn any new piece of software unless I really have to, because I spent too much of my life learning something and then next year its redundant. I mean, Minerva is a prime example. This year theyve got a complete new interface, so ive had to relearn, ive got everything set up last year and then try to copy it across, it doesnt work anymore, have to start again from scratch. So, in the same thing for wecco, learning exactly which button to press probably isnt worthwhile because the next version of weccer will have button in a different place, maybe.

Okay, proposal, we decide what we can be, and then to go in a bigger scale, like for more generalized or we not need to, what we are proposing does work at a small scale. I think youve managed to pack several questions in there, so I think at the end you said yes, managing to prove them small still that is a good idea.

So for a research proposal to epsrc for the to get million pounds off them, they often like the proposal to have have a small prototype which they've demonstrated works and that may be a bit too much to ask of you, but that will be very impressive if you can and to demonstrate it depends what youre doing.

So I might, for example, for my project, I have actually tried using chat ttp to generate the coursework specification and generate some of the questions for the multiple choice question tests. It came up with answers that were not really appropriate and made up wrong answers sometimes. But at least it came up with a draft which I could then improve on. Yes, you can.

Whatever your task is, you start off asking, you come up with a method and then you choose a dataset to try it out on. That's one approach. Alternative, rather than coming up a method, you might come up with a task or a challenge. For example, offensive text identification. They didnt start off by saying you have to use, I don't know, neural networks or something, but rather here is the task. Can you come up with a way of identifying, but it's offensive. And then people try different methods and they've got a score because there was a standard scoring method. So you might want to try, first of all, what is the problem youre trying to solve? And then think about what methods you might try for that. The day to 6, I just think for your education, rather than you one department, we need to just access that one today.

Well, okay, remember, youre writing a proposal. Youre not actually going to have to collect the dataset. I dont know the word random, by the way. I part of the plan is its a methodical choice of which data set. So I said, I want to start off with teaching and learning in the school of computing, because im fairly sure people here will cooperate because they like AI and computing. And then I want to methodically choose, select one school from each of the different faculties. And ive chosen psychology because I know someone in psychology and they're likely to cooperate with me and so on. So dont say youre just going to choose some random data. You have to have a reason for choosing your data, and it has to be appropriate to whatever the challenge is.

So what was the rest of the question? Okay, for now, but this one, scary or the? You just joined the module. Okay, you're in a hard place because the module is halfway through okay, there is this spreadsheet and your name is on the spreadsheet presume because it was created before you joined. Okay, so the options are you could do it on your own you're probably better off if you can joining a team, which means you got a find a team who is willing to add you in some of these teams have only got have got less than 6 on them so you could look through the spreadsheet and look at the smaller teams and email them or contact them and say, please can I join your team or at the end of this lecture if we leave early, anybody any team here who wants an extra member, stay behind and ask the guy in the yellow at the back, would you like to join our team? That would be nice.

Okay, question. If you decide to do merchant, that is very specific, very small scale. There is not a very, the scope of monetization we work on this project or is very specific about will be used a lot. I mean, would you care to give me an example? I don't understand what it means by small scale. So you might say collecting the course, but specifications from the school of computing is quite small scale because theres only maybe 100 modules or something like that. Its not a big data set compared to the worldwide website so if you take us anything from you or just. Specific modules, of course it would be a very small data set and will be serving for only one module. So can we take a project that is very, yes well what ive suggested is the first prototype is the small scale thing but have at least one or possibly 2 second rounds where you aim to do something on a larger scale. So you want to scale up typically in a data mining project you start off with a small data set to explore it and understand it and then but the reason for trying to do it small is that it's quicker and faster. But the reason for understanding it is then you can scale up to or try to scale up to something bigger. And youre only writing the plan, youre not actually doing it. So thats reasonable. Okay that's it. You're all quite happy and know what you're doing.

Well if there are any questions further you can post in the discussion forum. Nora Abbas, she is the co lecturer on this module. She'll also be answering some of these questions. She's on holiday at the moment. If theres nothing else you can all go home early except if you want an extra team member. Then theres a guy at the back in yellow who is looking for a team to join, and I'm sure he'd be very happy to join you otherwise. Thank you very much. Have a nice weekend, and I'll see you again next week. Bye, bye.


Right, hello, can you hear me?

Can you hear me now?

Hey, excellent. Hello everybody. I'm Eric Atwell. I am the lecturer for data mining and text analytics.

I shouldn't really say very much until 5 minutes past the hour because typically you are supposed it takes 5 minutes to everybody to come into the lecture theatre so I'm just holding fire. I played you some music if you want to hear more and this is my favorite band is the Clash from long time ago, from 1979 and it's on my website. You want to hear some more Google Eric Atwell and you'll find I have an arrangement with Google that I'm the first hit because there's an Eric Atwell who's a a web designer and as various other Eric atrocs. But I fixed it so that I'm the top of the list. So you can find my web page and watch the video. Okay. I'm actually.

Here.

To talk about data mining and text analytics, right? That's this.

Course. So.

Anybody thinks there shouldn't be here? As far as I know, this is an optional module. So today you can decide if you want to do the course. And if you decide you don't like it, then go and do something more interesting instead.

Makes less marking for me, but I don't mind. Im very happy to have you here. So the course is as well. See in a minute I have, I'm also currently teaching the same thing to an online masters in artificial intelligence and the digital education service helped us organize lots of teaching materials so I have im using much of the same teaching materials, ive put everything on Minerva for you to watch and read and stuff at the very top level.

Every week I'm going to be giving a sort of summary of what's happening this week, there's 8 weeks or 8 units and each week there'll be a topic that were covering data mining and technologics is a very broad area. So ive just chosen a handful of things which I find are the most interesting and hopefully useful to you in your future careers. Some of you may go on to be researchers but probably most of you go into industry and maybe use AI or data binding in some sort of way. So the what I'm gonna do is essentially go through the very quickly the lectures for this week. So there are 3 lectures that are prerecorded. You can look at them, watch them online. You can also look at the transcripts. You can also see the slides and so at the top level there's this weekly summary and then for more detail watch the actual lectures. For even more detail you can read the background reading because many of these are based on a textbook or based on some research paper, you dont have to read everything unless you want to become a professor of computational linguistics eventually. But if you just want to pass the course, you can probably get by with just watching the lectures and thats it. And if you want to find some, find out, if you find some topic particularly interesting, then theres more material for you to dive into this.

Honestly, although everything is online, you are at liberty to take your own notes. I see at least one person with a pen and paper. And traditionally what we used to do is write down the important bits on a bit of paper while we were learning. You can do that or you can put it on your computer if youve got a computer. So I've actually got my computer out with me and so I can watch myself in the zoo. But it says I have to sign in first, so I'm going to watch it later up. I should also say you hope its 5 past so I can start talking about the important bits. The university is in dispute with its staff and lots of staff are complaining that we have too much work to do and not enough time to do it. And therefore there will be a series of strikes over the next month or 2. And that means that some lectures will be canceled. Im not quite sure, I cant tell you it depends what modules youre doing, but theres another reason why I put everything online so that you don't have to cross the pickets to go to the comp, to the lectures. You can just watch them online. Even this weekly live lecture is essentially an option for you. If you want to come to the live lecture, great. If you dont want to then watch it at home. I just reply to on the Minerva discussion forum, somebody has asked, do they have to come to the lectures? No, you dont have to. If you want to, you can just watch it at home. Or if you want don't want to, you could just not watch it. I mean you're paying your fees if you don't want to watch it. That's up to you, right? Okay, as far as passing the course, there will be a test in week 4. I think it is an online multiple choice question in test and then another one at the end of the course. And then I'm also going to ask you to do a group project. And the project is okay. On the course im going to be introducing various technologies for data binding and text analytics. This week it's sketch changing. You have to come up with some practical research project which uses some of these tools like step changing or record or chat, gtp or Bert or some of the other things were looking at uses them for some practical application. So you have to first of all choose a group of friends to work with and then as a group, think of an interesting research topic that you might use these for. It could be, for example, your MSC project could be, so you could basically have to write a plan for it up to 12 pages long, and therell be specifications as direct actually what you have to do. It could be an mse project, it could be a larger scale project. Ill just talk to the pro vice chancellor for education, Jeff Gabriel, hes very interested in using AI and chat gpt for university education and we might apply to the engineering, the physical science research council for half a million pounds for a big research project over 3 years. So its up to you what scale you want to do is that it could be a sixmonth mse project or it could be a whole.

The specifications will be coming soon. Theyre not online yet because, well, I have to get clearance from every module. I have an assessor and the assessor has to check to make sure that ive written it correctly and it is readable by someone else. But it will be up there next week.

Okay, so the structure of each module, each unit, every week there's a number of lectures because its recorded. Some of them are long, but many of them are much shorter than 20 minutes or so. Dont worry, if theres 3 lectures, you probably won't have to spend 3. You certainly won't have to spend 3 hours watching them, because they're all quite short, or most of them are quite short. And this is textbook by dan jarassky and James Martin, and there are some other things for you to read.

Okay, I'm assuming that you've all gone into Minerva and seen this. Has had anybody not locked into Minerva yet and seen the learning resources? Has anybody gone into maneuver and seen learning resources? Okay, good. I'm glad you can hear me. So that does mean nobody put a hand up first, which meant either you hadnt or nobody understood what I was saying. Am I talking too quickly? Good. I will go faster and faster until you top stop me. So if I go too fast, somebody put your hand up. 

Because I tend to get enthusiastic about this stuff because its what I really signed up to do. Okay, so the first unit thats this week. There are a number of things you should have looked at and I have just downloaded the slides. So what im going to do now is very quickly go through this and I may at some point out some mistakes. I said these were all prerecorded and ive looked through again and ive realized theres one or 2 things that werent quite right. So watch out for these are deliberate mistakes to keep you on your toes to make sure that youre paying attention. So the first one was an introduction to the module. So I'm Erik Atwell and I'm going to give you another...I've just given you an overview of the volume structure. Do it again and talk about the assessment and then talk about the text you should be reading. Im a professor of artificial intelligence for language. That means we do things like understanding the quran or the bible, understanding Arabic or other foreign languages, how to develop chatbots for various purposes such as university education. How can they help?

Maybe you can replace a teaching assistant by having an intelligent personal assistant online instead. And I also have 40% of 2 days a week. I have a project with the Leeds institute for teaching excellence on decolonizing reading lists. And since there has anything to do with this course, I want to talk more about that.

The module overview. Emmm. Is this week an introduction and I've asked you to have a play with one tool called sketch engine. So some of you may be doing machine learning or deep learning. Anybody here doing machine learning? Anybody here doing deep learning? Okay, so the big difference is, as I understand it, in machine learning and deep learning, youre looking in detail at the algorithms, how they work and youre building, doing some coding in job in python to get these things working. On data mining, the perspective is much more and what is the input and what you do with the output to do get some useful results out. You treat the machine learning as a sort of black box.

And if you want to optimize the machine learning, you tweak the parameters and try it again. So its much more in data mining is much more practically oriented about using machine learning rather than developing machine learning.

If you want to go on to do AI research in machine learning, then obviously you should be doing a machine learning course. If you're thinking of using data mining and machine learning in a job in the future, maybe this would also be relevant. Okay, in the second week each week im going to ask you to have a play with some tools. These are not summative assessments rather just have a go so you have an idea what its like. Then you can decide if you want to use it later on either in the final project report or even in your future career as a as an it professional.

The second week on to look at a tool called weccer and also the methodology crispm, cross industry standard process for data mining. 1/3 week, I'm going to look at some of the underlying challenges of text analytics. That is, you want to be able to understand the meaning of a text, not what the characters are, but you want to understand that the word wedding and the word marriage mean the same thing, even though the characters in them are different. So how do you represent? And for machine learning, machine learning only really works on numbers or vectors of numbers. So youve got to somehow convert a word or a sentence into a vector of numbers, and then you can do all the standard machine learning and deep learning.

How does that work?

And also how you want to do it with very large data sets. So chat ttp is in the news at the moment, and that's trained on the entire internet, everything in English. And how do you scale to big data?

Now week for a break, there will be a test and I'll give you a chance to get working on your root project. Week 5, look at some of the application areas of this machine translation and also information extraction, which is how you ask a question and he somehow gets the answer from all the text. And also look at some python programming methods for text analytics, because so far looking at sketching in records and using tools. And week 6, looking at data clustering, data association, and my favorite topic at the moment, using chat bots of various sorts for university education.

And finally, at the end, week 7, have a look at some current research going on, like Bert and very large language models. And then after that, a final test. Notice the term finishes in week 9. So by then you have all finished, except youve got to finish off the project report to hand in after Easter.

So I realize a lot of other modules have a big piece of coursework at the end, like week 9 and 10 was some, I'm leaving week 9 free for you to get on and do that. I recommended this textbook by dan giraffe at Stanford university. He and James are working on the 3rd edition. If you go to the bookshop, youll find only the second edition. The library only has a second edition, which is now a bit out of date, but they havent yet finished the 3rd edition. So what dan has done is he put it online on his Stanford website. So you can go to the sandweb website and download the PDFs of all the drafts of the chapters. And theres also PowerPoint slides to explain material. So ive actually used some of these PowerPoint slides and adapted them a bit to make them more suitable for leads. There is also were going to be using other tools. And for each of these tools, theres some source material that you could read to find out more about it.

For example, for sketch engine, ive asked you to have a look at a paper by Adam cogaro who wrote the sketch engine system for weccer. Next week, there is a textbook by Ian Whitten and his colleagues at waikato university who developed that. And we don't have to read the whole textbook. It's just there as reference if you want to find out more. And im going to be asking you to read other things.

As I've already said, there's a test coming up and then another test is a mistake. I think each of our tests will be one hour. Actually, there's only 20 questions and then multiple choice questions. So you can do them in 20 seconds if you want to. You shouldnt really take an hour. You probably can do the whole thing and as long as you know the answers in about a minute, I think I can do it in a minute anyway. Maybe you could do it in a minute. But for those of you want to take longer, but please do not do it with your friends together. Each of you must do this individually on your own and dont try asking chat tpt for the answers, because ive tried to make sure that he doesnt know and I meant to ask this, I've talked about chat gtp because it's popular at the moment. And as I said, Jeff Gabriel is interested. Who here has actually heard of chat GDP? Who's not heard of chat ttp? Nobody says no. Who's actually used chat ttp? Okay, you've all used it. What for? Can anybody, can anyone tell me a useful thing you can do with chat gtp apart from just playing? Oh, have you actually done one?


You've written an mse project idea. Excellent. Thats a great idea. Okay, so you can use it for the coursework, but theres a caveat. Well, see in a minute. If you use it, then you have to write an appendix explaining how you used it. So the idea, the trick is you've got to think of a clever way to use it because it's actually like Google search. When you use Google you think it can find everything you want, but actually you've got an idea in your head of what you want. You have to think of what words to ask Google to find the web pages. And typically it doesn't get them all right? So you have to figure some other words. In the same way for chatting to p. The clever engineering part is thinking what is a sensible question? Whats a sensible prompt to get it to give you the right answer?

Okay, it says 7 pages here. Ive actually asked for up to 12 pages because I want this appendix telling me how you use chatttp or Google, whatever, to get your answers for now. Then get together in groups of 4:6 and think about what you want to do for this. And then the actual specification will come hopefully next week. If for some reason you really don't want to work in a group or you can't work in a group, then email me and we'll sort something out instead. And my reason for wanting to work has got at least 2 reasons, that the British computer society is very keen on group work, because they say if youre an it professional, you will be working with other people, you probably will be working in some sort of team or group so its a good idea for students to get some experience of doing this and therefore it seems a good idea to do it for a coursework.

The other thing of course is it makes it easier for marking because there's only 1/5 of the assessments to Mark. But that's not the real reason is for your benefit not for mine. Okay so ive already said this machine learning is mainly about the algorithms and optimizing the accuracy or whatever. For data mining its much more applying trying to find machine learning is only part of the overall problem solving and you have to worry about collecting the data for your machine learning assessments, you probably get given some data and then you have to use it. But for real life, you actually first of all have to think about what is the data? How can I get hold of it? Maybe theres already some data I can reuse. Thats quite a lot of problems. And there's also, it said that data analysts typically spend the majority of their time.

Data wrangling, that is, getting hold of a data and getting into the right form, compared to actually doing the machine learning and.

CRISP-DM methodology. Who's heard of CRISP-DM before? A handful of, OK, I wasn't sure.

So is that.

Well, some of the other modules, because you're all doing different combinations of modules. It may be that some of you heard of this before. This is the cross industry standard process for data mining and its the sort of procedure that you should follow if youre doing a data mining project. And notice only one part of it, the modeling, is actually running some software. Before that you have to figure out what is the customer wants, how do you get hold of some data, what does it mean, how do you get it into the right format? And then you can do the machine learning. And then after that, it's still small, you have to evaluate the results. It's not just which is the highest accuracy, but what's actually useful. And then you have to deploy, you have to put it into practice.

In some way.

So this is the sort of thing for your project proposal, for your MSC project or for this coursework. You have to come up with something like this, a set of sections with numbers of what is it you're going to do rather than just which machine learning algorithm are going to use. Text analytics is essentially data mining but apply to text. So most of the data in the universe at the moment or on the internet is basically English or other languages. Its not tables of numbers or images. There are images out there, but most of the useful information on the web is text. Therefore, suddenly computational linguistics or natural language processing or speech of language processing or corpus linguistics, these are different names for essentially the same thing. That is taking text data and converting it into a form, into some sort of vectors of numbers so that they need to apply machine learning on it. And then you get out interesting results in some way. And the underlying idea is a corpus is some collection of text for your particular application, and it has to be mapped onto vector numbers or number of vectors, so you can do machine learning.

I've always asked you to have a look at this book, the British council, they're sort of like the arm of the British embassy in countries around the world, and they sell great Britain, they advocate how great Britain is. And in 1999, which is now over 20 years ago, they had this big initiative called English 2000, which is promoting English language and great Britain in the year 2000. And they asked me to write a sort of a comic, almost.

A.

Report on where language technology was going. And some of the ideas are still relevant. For example, the underlying theory that language science or linguistics is divided into phonetics, which is about speech analysis, lexicography, about the words in the language or shape of the words. Syntax is about the grammar or how words are put together into sentences. Semantics, very important, is about the meanings of the word. You don't just want to treat them as strings of characters, but they are packets of meaning. Pragmatics is how the language is actually used in practice rather than just being technically analyzed. And discourse modeling is how you have interactions, like chats or conversations. And a lot of applications involve interactions rather than just analyzing some.

Language.

And thats still true today. Another reason the book is still valid is it talks about, in general, what are the practical applications. Things like, obviously, linguists and researchers want to have come up with computer models, but some of the practical applications are things like helping communication between people or helping communication between people and computers machine translation the social media and stuff like that and and what are the challenges? Well nowadays like a lot of computing the biggest players like Google and apple and Amazon and Microsoft they have huge computing resources. They can build things like chat gtp even if it takes farms and farms of computers and weeks or months of compute time whereas us on our own laptops we cant really do that sort of thing but we can log into their systems and use them maybe so that closes. Down right okay and and also its often difficult to elicit user requirements so for your project, you dont just go ahead and implement something, you got to know what its for. And that means you got to talk to users and figure out what they want. And they typically expect it to be perfect because when they speak to people, the people speak perfectly. Which is unfair because humans, even humans aren't perfect. So why should chat to be perfect? Okay so the report also talks about.

Research.

In this area. And in the UK, most of the research is funded in the British universities is funded by the engineering and physical science research council. So computing is in the faculty of engineering and physical science and most of the research in this faculty is funded by it.

The European union used to also fund, for example, we had a project called edgbots or chatbots in higher education along with some other European universities. And then about 3 years ago, something happened and Britain decided to leave a European union and stop accepting money from the European union. So were not allowed to be involved in this project.

Anymore. And.

Still, never mind that the book also has some interesting predictions about whats going to happen into the future. And some of it happened and some of it didnt. They thought that robots can take over any job at all in the home or hospital? Because the work in the home is menial work that doesn't require intelligence. Therefore, robots can do it. But then they, has anybody here got a home robot that runs their.

Home for them? You do, what do you do?

You read about it. Okay. But it turns out actually running a home does require a lot of intelligence. So I think they probably, there's some correlation between women running home and the Assumption that homes don't need any intelligence to run. But that turned out not to be true. It turns out actually running a home does require a lot of intelligence and the robots aren't up to it. I mean, a robot can run a washing machine but it cant actually put the washing into the machine or take it out and hang it on the line and that requires intelligence like I know to put, I've put mine out today because it's not raining across fingers for example.

Okay, it's also got some lots of interesting examples of tactical applications. You might want to look at some of these because this is the sort of thing you might want to have for your research project. You want to think of some practical application for which you can use sketch engine or chat ttp or one of these other tools, or maybe 2 of these tools preferably to at least 2 of these tools for your particular application. Okay, that's a summary of what was in there and I'll get rid of.

That.

Any questions? The whole point of this live lecture was that at least those of you are here, you can ask a question. Those of you are watching at home afterwards, I don't know where the camera is, the camera.

Up there somewhere.

Those are watching a home afterwards and you can also put things into discussion forum and in manoeuvre there is a discussion forum. And at least at least 3 people have already posted. So be ahead, go ahead and I'm 2 of those one but at least one other person has posted something. Youre welcome to post something there. Okay, the next one. No, just on.

Her. Go away.

The next one. Right. So every week there'll be at least one, if you like, technical lecture, which is based on a chapter or section from the Jurassic amp. Martin textbook. So this is all the slides in this sort of format. These.

Are.

Girafsky and Martin sides. So this is, if youre doing text processing, then the very first thing you have to do is some basic preprocessing to get it into some sort of format, which you can then start doing something interesting in. Now, I havent got time to run through all of this in great detail, so if you want to get the exact detail, then watch the lecture.

Online.

See, this is quite a long one, whereas the every week there's also some more less technical lectures, which generally are much shorter because im going to run through this at rocket speed. And if you see anything you dont like, put your hand up or shout out, right? Well, those are online. You cant put your hand up, but you can at least put something in.

The discussion forum.

So the first thing is regular expressions. And that is in English there are many words and they are more or less the same word, like woodchuck, would chucks, wood chuck, capital w, and woodchuck with a lowercase first letter. And you want to be able to write a pattern which captures all of them together.

Let me just ask who has come across regular expressions before, before today? Many of you have. So this is a standard feature of python or of Unix Linux, right? You've got these features. So it's not really rocket science, but it's worth knowing about partly because it's a very simple example of some very basic text processing. And you can have other patterns like a to z matches any capital letter, or a to z lowercase matches any lowercase letter. And theres some more patterns such as something, the carrot symbol means.

Not.

And you can have not hsn matches anything which is not a capital letter. If you all seen these, anybody noticed anything wrong here? No, I, this is one of the slides from dan giraski. I was gonna email him and say, I think you've made a mistake in this. Can anybody spot what the mistake is? Okay. Is it.

My fault? Yes.

The car on the first one, 3rd one, I think this says carrot, e, carrot, that carrot, h is ed means not, h is ed, carrot capitalist littlest means not capitalist and not s.

Littlest.

Carrot, e, carrot should mean not e and not carrot, neither e nor carrot. But the example is given is, look here, the letter e in here, and that's not, that shouldn't be allowed. Any of the other letters would be an example, but except e and e are not pat. I think that's right. I think that's a mistake. Anybody agree with me? Anybody disagree? Okay. Well, go away and think about it. If you think this is right, please put something in the discussion forum before I send off to Mark, to James Martin and dan giraffe. I'll leave it to next week. The point is none, just like chat EP, none of us are, we're all human and none of us are perfect. So it's not.

His fault.

And in fact, that's why he's put his stuff on the webpage so that people like you and me can email him with the mistakes before it goes to print. So you're doing the world of service by helping out. There's some more examples. And this also illustrates the general point in natural language processing and in pattern matching. Generally in machine learning, you start off.

Finding.

You start off with, if you're doing some experiments to find the best machine learning algorithm, what you do is you start off with a baseline or very simple machine learning approach, which works most of the time. So if you want to find all the examples of the word the in a text, then a simple pattern is lowercase t, lowercase h, lowercase e. But this doesnt work over time. It misses out if the first word of a sentence is the then it wont capture it. And this is an even more complicated pattern which also doesn't misses out other or theology or other complicated cases.

So this is an example of error correction, not just for this purpose, but in machine learning experiments for example, for any sort of experiments, you start off with a baseline which is very simple, but makes mistakes or isn't very accurate. And then you work out what it's doing wrong and then you try to fix those errors. So there's definitely 2 types of errors. First of all, if you if this figure is just the, but it will match things like there and then and other and these are false positives. It's a positive because it's matched but it's false cause it shouldn't have matched. And thats one type of error. The other type error is its missing is not finding thc at the start of liesitance. So that's a false negative it should have found it but it hasn't. So its predicted as negative predicted as not being the but its false because it is actually the. And these sorts of errors occur all over the place. If you do the Google search and you want to find all all the information on using chatbots in university education, then I might type into Google chat bot education and it will come up with a list of hits. Some of those will not be relevant, some of those are false positives, some will be that maybe a YouTube video about trying to teach a chatbot, which is not what I want, right? And that's a false positive. But there'll also be other things like that. Our deputy vice chancellor for education has a blog which talks a lot about stuff. If that wasnt in, if Google didnt find it, and thats a false negative because I did actually want to read his blog, but it hasnt found it, it said its not relevant. Okay, so.

In.

Text analytics, generally, youre trying to increase, you start off with a simple experiment and you try to improve the algorithm by getting rid of the mistakes. And you can either improve precision by minimizing the false positives, or you can improve recall by minimizing the false negatives. Typically, what you do to fix whatever it is will have either, I will either find, if he improves precision, it will minimize 4 positives, it will get rid of, it will stop showing you things that you didn't want to see, but you still miss things that it hasn't shown you. Or you can increase recall, which means all the things you want to see, it shows you more of them rather than improving the.

Accuracy.

Okay, so thats that. Here are some, very quickly, some examples of practical applications of these regular expressions, one of which is Eliza, a very simple early chatbot. Eliza the chatbot has a very simple pattern matching mechanism, which is more or less regular expressions. You say something and it replies to you by seeing what you typed in and coming up with an appropriate pattern. When I had a mental illness problem, my GP referred me to a psychotherapist and I said to her, stuff like, I don't know, I'm feeling miserable. And she say, why were you feeling miserable? Well, my wife told me to come today. Why did your wife tell you to come today? So that's actually what psychiatrists or psychologists have forgotten, which one is what the mental health advisor is supposed to do. They're not here to talk. They don't solve your problems. They help you to solve your problems for yourself by reflecting back on you what you're saying. So although this is very simple, algorithmically is actually a very appropriate model of a psychological help person. So if I say I'm depressed and it comes back with, I'm sorry to hear you are depressed, or it mimics back what youre saying. Okay, next issue is how many words there are. If you're converting text into vectors of numbers, then you have to count up things. That's what numbers are.

That counts.

So the fact, an obvious thing to do is given a text like a sentence, how many words are there in a sentence and then for each of the words in a large corpus, how many times do they occur? And that depends on what you mean by a word.

So here's a sentence, I do mainly business data processing. Well that looks like I do a mainly business data processing. There's 8 words. On the other hand, you might say mainly, thats a pause. So thats just one word, ive just done it. So maybe it's I do earth doesn't count, main doesn't get mainly business data processing, thats 6 words. But then business data processing, thats one thing. So it should be I do mainly business data processing, 4.

Words.

It depends what you're counting, that's the point. Also sometimes the same word appears in different forms like Susie's cat in the hat is different from other cat. Cats is cats and cats the same word or different words. So really they're the same dictionary entry but there are different character strings, there are.

Different.

And tokens but they're the same word type. And there's more examples in the lecture online. The point is a type is a dictionary entry or an element of the vocabulary whereas a token is one instance or one example of that type in the text. And depending on how you define it, there are 15 tokens or possibly San Francisco cancers one tokens that is only 14. So another problem is in English you think where the spaces are that separates the words. It doesn't always work like San Francisco or New York. Anybody here heard of.

New York?

All right. Okay. Anybody here being to New York street in Leeds? No? Okay. Anybody here being to the market in leads? Yes, right. The market in leads is on New York.

Street.

And it used to be on York street, the road going to York, but they built a new road and now it's called a New York street. So that illustrates that even New York is ambiguous between one word or 2, depending on the context. If im catching a flight to New York, thats one word. If im going to New York street to go to the market, then its 2 words, right?

It depends.

Okay, so if youre collecting a data set, then you can count up a number of tokens and also the size of the vocabulary. And there's some sort of relationship between the 2. The larger the text, the more words there are is the more tokens there are and also the larger. Its not an exact number because it depends on who wrote it and how varied their languages.

For example, the complete works of Shakespeare are about a million words, not quite, but nearly a million words. And Shakespeare was prolific. He wrote lots of stuff. I would tell you, anybody here heard of Shakespeare? Anybody not heard of Shakespeare? Good. Okay. Shakespeare is, is, was a, someone who wrote lots of stuff, okay? And he wrote a million words of stuff and there's about 31000 different words in it.

Now my first job after I graduated was to work on something called a lob corpus, a Lancaster Oslo Bergen corpus of British English, which is like a million words sample of British English to compare against the brown corpus of American English, which is a million words of American English. And in that there are about 50000 different word types. And you might say, hang on a minute, Shakespeare is a brilliant author. How come he's got fewer words than the lord corpus?

Well, that's because the lord corpus was collected from lots of different sources, from newspapers, from works of fiction, from government reports and so on. It was actually written by lots of different people in lots of different genres or text types. And therefore there's more variation in vocabulary. There's words in the government reports which didn't appear in the newspapers, which didn't appear in the fiction, where Shakespeare was just one man and all he wrote about was love and war and comedies. He didnt really do government reports or a lot of other stuff, didnt do sport stories or whatever. So that's why lob corpus has got more variety and therefore larger vocabulary.

A corpus, if youre collecting a corpus its for a specific purpose so you have to decide if part of a data mining task is how, what's in the corpus, what you need, you may want to choose a particular writer or author, particular time variety, language or whatever. For example you might want to capture a British English corpus rather than one of the other languages and it's British rather than American. And it only contains work by British authors, not by others for example and for log corpus it deliberately included news stories and fiction and scientific articles. It didn't include Wikipedia because that didn't exist at the time. And you also have to keep records.

Of.

The authors, where they came from and so on. Because just collecting, when you say collecting British English and how do you determine what counts as British, and therefore you might have to keep lots of metadata.

About this.

Okay, having got the text, then you have to count up the words. And what counts as a word? Well, you have to divide it into text words by essentially looking at where the spaces are. That works reasonably well for English and Arabic and some other languages. It doesn't always work. But if you're anybody here, use Linux or.

Unix.

So if you use Unix or Linux, then there are in the Unix shell, there's nice tools for doing this. And someone called Ken church actually wrote a paper on Unix for poets, where you show some examples. If you take the complete works of Shakespeare, and then this line of code will take the complete works of Shakespeare and met everything, all the letter, all the, wherever theres a nonalphonumeric character, it replaces that with a space. And that means it takes the text and chops it up into words, and you get basically one word per line, and then you can sort it into alphabetical order. And that means all the a's will appear together, and then all the odd box will appear together and so on. And then unique minor c that counts up all the occurrences of each repeated word. So that fail, you get a count of how many times you get a, how many times you get odd voc, how many times you get abacus, and so on, that merges and counts each type, so you end up with something like this, a word list of all the words and the frequencies of all the works. Okay? And if you do this with Shakespeare, then as I said, the first thing is to break it up into one word per line, and then you sort it into Africa order so that all the a's appear together, and then you have to merge upper and lower case, otherwise capital a will be separate from lower a, and then you sort all the counts and then you find hey presto youve got 23000 occurrences of the and 22000 currencies of I and so on.

And as with all examples in textlets this doesnt work perfectly. For example the word appears up there is 8000 times. That's because I said wherever you have a non alpha numeric character you replace that with a new line. Any. So basically the words like I'd like, I'd love to give you a kiss. I'm sure that appears in Shakespeare somewhere. I becomes I and dirt. So I is still a word but dirt is a separate word. So this is another problem. Dirt is really do but it doesn't represent it as do. So you cant just blindly remove all the punctuation because theres various cases where the punctuation is actually important. And you also have words like we are in English or GE or loner in French. Anybody who speak French? No French speakers? Okay. Well I can mispronounce the French and nobody will know. That's good. Okay.

And there's also these multi word expressions like New York, is that one word or 2 words? Well, it depends on the context. Within later on in week 3,4, I'll be looking at python tools for doing text analytics. One of which is the natural language toolkit developed as a bundle of python tools for doing various things. And heres an example to have tokenizers. This is a regular expression tokenizer, which has a very complicated regular expression for matching words in the text. Even more sophisticated, you have whats called a stemmer. Well, look at a stemmer in a minute. But that works for English, for other languages, are not so easy for Chinese. Can anybody here read Chinese? And nobody wants his to be Chinese. Okay, I'll read it, shall I? It says Yao  reaches the finals. Did I say that.

Right? Okay.

Okay, I'm cheating. I can't speak Chinese. But the point is it's not obvious if that's maybe 3 words or maybe it's 5 words or maybe it's 7 characters. And every character represents the word. So in Chinese, sometimes a word is 2 characters. It's a bit like in English, reaches is really one word or maybe it's reach and s or finals is final and s maybe is one word or 2. And the same thing for.

Chinese.

So in Chinese it's not as straightforward and other languages like Thai and Japanese are also problematic. How does weve send chat gtp in these very large language models? They dont actually have, they sidestep this issue completely by using something called bike pairing coding. Rather than trying to segment the text into words by using the spaces, they use a much more sophisticated method which I hadnt got time to explain now but maybe it will come up later on. So all this theory, what they've done, instead of using the theory, they've got a huge neural network and it does it automatically.

Okay another issue is you should try to normalize all the text, putting the capital letters into lowercase. You need to try to work out the how the word breaks up into words or intersections or segments called morphemes. And a simple way to do this is to stem. So an nltk also has a basic stemmer. And a steamer basically chops off the end of the word on the assumptions. The ending is important. Like s usually marks a plural. Therefore, if you chop off the s, then you get a stem. Unfortunately, in this case, its v is a stem and you might think v isnt an English word, but it doesnt really matter so long as it maps all the different words into the same form. For example.

Where's another word?

Accurate is chapped onto Acura. Acre isn't an English word but also accurately and accurateness are also chopped onto acre. So long as all the different variants are are mapped onto one form it doesn't really matter what the form is. It doesn't have to be an.

English word.

And theres the porter stemmer. It has a set of rules for doing this which come in the system. For some languages like Turkish it's even harder. I'm not going to go there. And also there's a problem of sense, not just segmenting words, but segmenting sentences. So you might think whether there's a full stop at the end of the sentence, it doesn't really work like that.

All the time.

Okay, that's.

That.

And finally, let's get rid of that. I wanted to show you sketch engine. Im going to run at a time to actually do sketch engine, but ID like you to have a go at using sketch engine. Sketch engine is a tool for collecting a corpus and then doing interesting things on the corpus. You can get things out like a word sketch. So this is a word sketch for the word catch. And it shows you for the word catch what are the other words which appear next to catch. And there sometimes catch is the subject, sometimes it's the object and so on. And this list of other words which appear around catch in a large corpus there may be thousands or even millions of examples of the word catch in the corpus and the set of words which appear around it that captures the meaning of the.

Word catch.

Because the words which appear around catch are also the, most of those words will also appear around the word grab, lets say and therefore the collocational profile of catch is similar to the collocation or profile of grab and that the frequencies of those words around it are what you capture in a vector to give you a vector representation, a numerical representation of the meaning of a word. You can do this for any language. You can do it for Arabic, you can do it for Chinese as well. And you can do it for phrases like court.

Pants.

Theres a phrase and you find that court pants usually comes with caught with his pants down, which means court in a compromising position of some sort. But you can also get variants like court with your digital pants down or caught with their pants down. So you can see examples of that. And if you've got this collocation profile, this vector of frequencies of other words which appear next to this word.

Then you can use it to generate a visualization like this shows the meaning of the word t. And the meaning of the word tea is a set of all the other words which come with tea in text, like coffee and drink and beverage and so on. And in this case, the size of the word shows you how frequently co occurs with t. And if we look for, you can draw another one like this for the word coffee, and it will look very similar to this one, except where coffee is itll have tea instead.

And this representation, this visualization, underlying the visualization is a vector of numbers. And the vector of numbers for t is similar to the vector of numbers for coffee. And that's because the meaning of tea is the same as or similar to the meaning of coffee. That's how you represent meanings of words as vectors of numbers.

Then you can do machine learning on those things. That as the key element. You can also do this if you have a parallel corpus. If you have lots of text in Chinese, translate it into English, then you can work out that the word smile keeps appearing in other sentences where this character sequence, which I can't translate appears. Therefore you can compute what is the meaning in Chinese of the word smile because its those pairs of characters which keep cropping up in sentences where the English word is.

Smile.

You can also use theres a tool called web bootcat for creating your own corpus. You have to give it a corpus name, choose a language and give it what they call it some seed words. That is some words in the topic area that you're interested in. So this is an example. This is a volcano specialist. He's put in some words to do with volcanoes and then he presses the button and it goes away. And it uses Bing or Google search to find web pages which have some of those words on it. And then it pulls the text from those web pages and that gives you a corpus and then from that corpus you can then use other tools within sketchchanging to find the most frequent words, the most significant words and these are words in the area of volcanology or the study of volcanoes, things like lava and eruption and volcano are in there and you can do this for yourself, for your own topic area.

This incidentally the school of physics here for example did it for physics terms. So they got hold of a corpus of past physics exam papers and they compared that to a corpus of general English and they found the physics terms which appeared which in the physics exams which didn't appear so much in the order in English, and someone else has done that for mathematics. You could do this for computing if you wanted to, for example. So scale changing, there's lots of these functions for doing that sort of thing. Here at least university, we have a Leeds internet corporate website where weve done the same sort of thing, but locally you can have a look at that too. There is also the association for computational linguistics, which is like the scientific body for text.

Analytics.

Computational linguistics. They have a number of special interest groups or sigs, and one of them is called sigwhack, a special interest group on webez corpus. And theyve got lots of resources there. You can have a look at. You can use sketch engine yourself. If you do, you have to use it on campus because we only have a license for on campus use. So if you want to use it off campus, you first will have to connect to the university VPN so that your computer thinks it's on campus or rather that sketch engine thinks it's on campus.

Okay, finally, I think I'll finish that. I haven't really got time to do this, but I was going to try and log into sketch engine just to show how to do it. There's the sketch engine web page. I've logged in and I haven't got time to do this now. So you should do this at home. There's a nice video on how to do it. The thing you want to look for is down here. There's, whoops, various stuff. You can choose a language, you can do all sorts of things with. You can create your own corpus. So there is no corpus collected. You can select a corpus and if you haven't got a corpus, you can create.

Your own.

Okay, if you're getting restless, I should say finally be aware that the staff union is in dispute with the university. So there are going to be strikes and some of the lectures may well get, end up getting canceled because people are on strike. However, I have put on the website the recordings of the lectures from last year. Should some at worst. You can watch the recordings of last year's lectures or you can watch the lectures online. So you dont have to come onto campus if you dont want to cross the ticket line just because it makes you feel bad. Then you can stay at home and watch the lecture at home afterwards. And if you have any questions asked now or else, post them on the discussion.

Forum.

Right, final question. Has anybody decided they dont like this after all and want to go do something else instead? Please think about it. I don't want to have to Mark 200 pieces of coursework. I'd be much happier if you went and did machine learning or something else much more interesting instead. This in China. Okay, that's it. Thank you.

Very much. Bye for now. Okay.

No problem. You've stolen your last snow days and last moment I'm scared. That's Chevy Chase Chris Elliott. Snow day. I'm gonna ask My friends here to give me just a Little push. I said another one.

Yes, just to say thank you for coming to university despite the snow. University advice, I'm told was that it was at all dangerous, stay at home. So I stayed at home this morning, but then the sun came out.

Can you hear me?

Can you hear me now? Can you hear me now? Oh, dear.

What's up man? Let's try this one. Hello?

Can you hear me now? Can you hear me now? Okay, looks like now there is a working.

Honey, let's go back to. Okay, can you hear me? Is it coming.

Through on the speakers.

Now? I guess that whoever gave a last lecture didnt put the thing back.

Into charge. I'll control that. May sort of work.

Okay, I'll just have to talk loud so you can hear me at the back. Okay, I'll try and talk with if you can't hear me, come down the front. There's plenty of space in here. It looks like maybe 20% of the class is actually here. Most people have decided to stay at home. Thats okay. For those of you who didnt come in, youre missing a beautiful snowy day and there's plenty of snow out there to make snowman and all sorts of stuff. Or have a what if you don't wanna go out, then you can watch the movie about snow day. Most of the schools around my area are closed. Theres loads of kids playing snowballs and going on sledges and things. This is your one chance. The snow will melt and disappear. So if you want to enjoy some snow, do it Today.

My daughter is going to drive out to the snow this evening and have Fun there. And This week, I think got as far as unit 6, which is about 2 things. So there's actually 3 videos that I wanted. The lectures that you should have watched or seen. First of all, on unsupervised machine Learning.

This is just solved for completeness. Most of machine learning and data mining is about taking a label dataset where each instance has a Label.

And then building a classifier, which given the training set learns a classification algorithm of some sort so that given a new instance, it can tell you what class it belongs to as classification. And most of machine learning and nowadays most of AI is about labeling data using classifiers. But you can also do machine learning in various unsupervised ways. So that this lecture gave you a sort of flavor of a couple of alternative approaches.

And then Chatbots are very popular at the moment, particularly since Christmas and the advent of chat gpt. Anybody can have a chat with a very intelligent chat bot and it can come back with really good answers so we want to give you a sort of background To that.

Finally there was a recorded lecture theres no PowerPoint slides with it thats because its I was just I? Was going through a research paper this paper here just to point out some of the things you might want to notice so we did had a research project with a couple of companies who develop chatbots and we use them on our classes last year one of which was a. Student survey chatbot so at the end of the semester youll be asked to fill in an survey form and we thought it would be more interesting if a student could have a conversation with a chatbot and a chapel would say things like what did you like about the lectures? And if you said nothing but he would say are you sure surely there was something nice about and engaged this student with a conversation.

Unfortunately the company has decided theres no money in university applications so they've switched and they now have chat bots for interviewing job applicants. So if you come for if you apply for a job here I mean the university of leases is currently advertising the hundred and 30 vacancies for staff including about 5 or 6 lecturers but also mainly management and administration staff. So if you want to you can have an interview but rather interviewing with a real person you can interview with a chatbot. Unfortunately the Student Feedback chat but isnt available but you can see a bit about it there. Okay, lets have a look to Start off with and Is unsupervised machine learning. What I'm going to do now is very quickly run through what's already in the lecture. So I'm going to if it's anything you don't understand you have a chance to put your hand up and say something, right? Those of you at home any questions put them in the discussion forum because you can't really put your hand up here. So unsupervised machine learning you have to know about it.

Who, let me just ask before I do this. Who's actually used clustering or association rules at all before? Okay, one person. Its as if all of you probably have used machine learning classifiers. Who's used a machine learning classifier to classify some data? Who's heard of machine learning Classifiers?

But nobodys had to use them. Its interesting. Okay, so there was in the second week I asked you to have a look at go using weccer and weccer lets you try out some classifiers. So I thought maybe youve tried them. Anyway, most of the machine learning stuff if you use it is probably classification of Some sort.

So hopefully by the end of this week you have an idea of whats the difference between classification and association Rules.

And have a look at least one example of an algorithm for association rules. And also you should know the difference between classification and clustering and we'll have a couple of algorithms For clustering.

So you have the idea of what's the difference between supervised machine learning which is given lots of examples and the class supervision. The supervision bit is the class. So if somebody has to decide what class agencies belongs to, that's like telling the algorithm what it needs to know and thats why its called supervised. Whereas unsupervised means you just have a set of instances and you dont have anything in advance telling you what is what class they Belong to Classifies essentially you have an instance and the instance is broken up into a set of features if it's a piece of text and the. Features might be the words in there in in in in the piece of text and then they have to you have to use the features to predict the class so the class is like an extra feature and its a special feature because the idea is that theres some correlation. Between all these other features and the Class.

So thats what all machine learning classifier does and theres different ways of doing it as decision trees and theres neural networks and so on, but thats all they do in effect. But the trouble is a lot of data out in the world doesnt have class labels. So if you just collect some text, it's just a set of features. It doesn't belong To a class. You can.

Generate these classes either by getting a human to label it with class a, b or c, or sometimes you can do it in a clever way. For example, if you collect some text from United States and then collect some other text from United Kingdom, then all of the text from United States can have a label us, and all of us text from the United Kingdom have a label UK. And therefore you've got a data set where each instance is labeled with either us or. UK and you can automate that so you can come up with labels but if you have all the labels then youd like to be able to say What.

Given all this data are there categories of things which are the same and those categories those instances are similar and those. Are similar and this lot are different from that lot so that sort of generating the classes automatically. That's what a Clusterer does.

You may also say the features in these instances and the features in these instances may predict the class, but its also interesting that this feature in this feature co occur in this distance and this feature in this feature also co occur in this instance. So there's lots of instances where the same sets of features come together. And that's what association is, finding things that co occur. If you have lots of information about shopping habits from baskets of people, then you can notice what products in the shopping baskets occur Together.

In the recorded video. I looked through a simple example in my PC lab. I've got a number of pcs and I want to buy some new pcs. In the past, some of them have had viruses. I want to know whats the likelihood of a new pcs getting a virus. And I asked my PC manager for all the existing pcs, what is the risk of them having a virus? And he uses his expertise, whatever it is I don't understand, p c. Says leave it up to him. And so he will do the labeling. He will label each of them as high, medium or low Risk.
And in Addition. We record what features of the PC are likely to be relevant to getting a virus. So one thing is if they share a file server, then maybe the virus passes through the file server to the other PC. So that could be relevant. Or if they both connected to a scanner, then potentially the virus can go from the PC to the scanner and then onto Next PC.

Another feature which is likely to be relevant is whether or not theyve been infected before. So it could be that if a piece has been infected before, but it's like to be infected again on the hand, we don't Know that. It.

Could be that if it's been infected before, then the managers be more careful about putting an antivirus software on it. So its less likely to be, we dont know if its more or less likely, but it is relevant Potentially.

So a machine learning classifier. Well, given a set of instances, each PC has got fs and I features and it needs to predict the risk for association rules. We just want to find any correlations between any of these Features.

So that means that there are many possible correlations between features as the search space is much bigger than just for a classify. So what we want to do is a very basic algorithm generate all the candidate possible rules that might apply and so long as they apply to several examples and then for each of those rules work out what is the accuracy of the rule. And then you have a load of rules and then maybe you just have a cut off and say I only want the rules which are very accurate and you can even go to the extremes saying you only want the rules which are 100 Accurate.

So there's the steps. First of all find all the combinations of attributes or features which occur together at least end times. And these are called item Sets.

So here's our example pcs and there's one 2345677 different pcs and preach room. I've noticed whether or not they share a file, whether or not they share a scanner and haven't been infected before. And also the managers assessment. Most of them are high risk. A couple of them are low risk and one of them is medium risk. He couldn't really Decide.
So what things cooccur together at least 3 times? What feature values can We find Which occur on at least 3 different pcs? Well first of all, these things called one item sets, theres a set of one feature which occurs at least 3 times. So Equals yes.

And that the share files, that one, that one, that one, that one and that one share files, there's 5 cases, so that is at least 3. So sharing a file is a feature which occurs at least 3 times. Trouble is, youre trying to find correlations between features. So one feature isnt very interesting. What you really want is 2 item sets, pairs of features which occur at least 3 times. So f equals yes and s equals yes. Theres one f equals yes and s equals yes. Theres another f equals yes and s equals yes another one says 3 pcs where thats the case for the other pcs? Theyre not both yes but thats the 2 items set which occurs at least 3 times and these other 2 item sets also occur at least 3 times you could also look for 3 item. Sets It turns out there's only one 3 item set which occurs 3 times.

And that's that one.

Okay, so thats that. Maybe there are combinations of these features in some way now having got the item sets then you want to generate the predictive rules. You're not just interested in things that come together but there's one thing predict another and if something then something else. Thats the Sort of rule you want.

And lets take start off with the not interesting example of just one feature they share a file. Well a possible rule is if true then they share a file and if true is something which is true for all pcs. So there are 7 pcs where if true applies and also 75 of them that one that one that one that one and that one have f equals yes therefore.

And That it that is 5 cases where f equals yes but the accuracy of this rule is only 5. Out of 7.

So it is a rule and it's quite an accurate rule but it's not 100% accurate. Furthermore it's not a very interesting rule just saying if true, then what we really want is if one feature then other feature. So therefore you want to look at the 2 item sets f equals yes and s equals yes. And we saw that one as applies, that one applies and that one applies. So there's 3 cases. There's a coverage of 3 for each of these Rules.

But the accuracy is actually different because the rule is the other way around. If s equals.

Yes.

How many cases are there where if s equals yes, that one, that one, that one and that one. Theres 4 cases where this condition is True.

And it offers 4, only 3 of them where f is also yes, therefore the accuracy is 3 out of 4. Whereas the other way around, if f equals yes, that is if they share files and there is one 2,3,4,5 cases where they share files, therefore theres a coverage of 3 where that is true, but that the condition is met in 5 cases and the rule as a whole is true in only 3 of them. Therefore, his acquisition 3 out of 5.

So you see, although these look the same, theyre not because theyre the other way around. This is an accuracy of 75%. This is an accuracy of 5 or 60%. So this is a less, this is a worse rule.

Than that one.

Is this clear to everybody? Anybody not understand? It's fairly simple.

Maths, right?

There is another possible rule, which is a very interesting rule, which members said, if true, that could be a condition. If true, then f equals yes and s equals yes. But this isnt a very good rule because weve already said theres only 3 cases where they both share files and share scanners. This is a coverage of 3. So this rule, if true, is true 7 out of 7 times is always true. Therefore, the actress is just 3 out of 7, which is less than 50% Sets.

And then you generate a whole set of possible rules. And theres all the rules for the 3 items set and then you haven't got all the rules, then you say I only need to sit in the rules which apply to what? Which are 100% accurate. I mean thats extreme if youve got a larger data set then you may want to have a slightly lower threshold. But in for this example if we only want the rules that have hundred percent, that means you prune or rule out most of the rules and you're left with just 2 Rules.

And the rules are the ones which always are always true, is if I equals no, if they don't share.

Sorry, what was, I Think I forgot now. I infected before, right? Sorry. If I hadnt been infected Before.

And the risk is high, then they are likely to share files. This isnt doesnt classify whether or not its at risk or not, is simply saying as a correlation between if you haven't had an infection before and the expert thinks the PC is still at high risk, and that's probably because they share a file server. In other words, that suggests that the file server is likely to transmit the virus, I suppose. And another rule, another correlation, if they do share a file server and they havent been recipient before, then the risk Is high.

And that's actually a classification rule. It happens to be a classification rule, but not all the rules are Classification rules.

Okay, and this is useful. I mean, that's not a a very realistic example, but it is useful. For example, Tesco, I have a Tesco Club card. Here we go.

Now, every time I go to Tesco, I scan this. And that means Tesco has all the data of everything I buy, and they have all the data from everyone, everything that anybody buys. And I know from that they can work out for baskets of shopping, what things people tend to buy together, and they've worked out, for example, that people who buy bread are also likely to buy milk and stuff like that. And then for my purchases, they can send me money off Coupons.

For things that I have not bought, but other people who buy my stuff do buy their stuff, and therefore they can recommend it to me. As much the same as Amazon recommended systems, they look to see what correlations there are between purchases you've actually bought and other people have bought the same stuff, what other things they've bought that you haven't. They recommend those things To you.

That's what association rules are for. Okay? Association rules don't group things into classes that they find matching things, whereas clustering is for developing groups of things that are similar. So here's a very simple data set. This is from customers of a travel agent in United States. And in the United States, if you go.

Abroad, you're likely To go either north to Canada or south to Mexico. There's not much else to go to because they don't like us. We haven't got lots of countries nearby. There may be a small percentage who are very rich and can come to Britain or China or other exotic places. And then if you have this data, theres only 2 dimensions. Theres the age and the country they travel to. You cant really see the clusters obviously from just the data, but you can visualize this by displaying in A graph.

And if you show age along one axis and country on the other axis, well theres only 2 countries, theres not really an axis, but you can start to see there are a cluster of people going to Mexico, young people go to Mexico, middle aged people go to Canada, old people go to Canada and there's also this Gap.

And then having got the data, this amount of data, you don't really need a clustering algorithm to see the clusters, you can just do this by hand and you can speculate as to what the reasons were. Having got the clusters, you can explain them or you can ask the travel agent to the expert may be able to explain them. Young people say their parents took them to Canada so they want to try something new. That's why they go to Mexico instead. Middle aged people, they take their children to Canada because it's not Nearby.

Old people go back to Canada to relive their youth. Maybe in between middle aged and old, the parents are busy paying for their children's education in universities so they can't afford to go abroad.

That explains that too. So that the machine learning cant give you explanations, but it can give you the clusters. That's if you've got a very small dataset. If you've got a larger dataset you may not know in advance. If you try to try to visualize a very large data set, you probably get something that looks like MUD or spaghetti or something. It's very hard to see clusters in a large data set. Its easier to see in a small data set. So what you can do instead is automate the clustering. But if you got automated clustering you have to know how many clusters you want. You have to. So for the data we looked at it looked like there were 3 clusters. That was easy. If you have a bigger data set its not obvious how many clusters they have. So an alternative is to come up with whats called a dendrogram. This is a from a Greek word for a tree Picture.

And this shows that at this level there are 2 overall clusters and theres a group of words that government, council, committee, party, development, education, a one cluster and the other cluster is water, food, law, music. And these are done by some sort of collocation analysis or something. But if you look at a finer level of detail, lets say at this level, if you cut it here, then weve got one cluster of government council, another cluster committee and party, another cluster of development, education, another cluster of water and food and another cluster of law and another cluster of music. So vedendelgan lets you decide at what level of granularity you want the Clustering.

And well, see in a minute how we go about doing this. But in general, to be able, anything like this, you have to be able to measure how close together 2 instances are. And to do that, you have to convert the instances, whatever they are, text or database information or something, its got to be melted down into a vector of numbers, because we can measure the distance between 2 vectors of numbers. You cant easily measure a distance between 2 words.

So heres n dimensional data about shopping at Tesco Whatever.

Each of these, some of these can be numbers, some of these can be other stuff, but to be able to measure a distance between 2 instances, you can use some sort of mathematical function like this. But that only works if you've got every instance is a vector of numbers, even instances of vector numbers, and you can subtract one from the other, square it, do a sum, do a square root of a sum, or whatever works for you. You could do some mathematical calculation like that. And so a vector representation is very important.

And there's an example of a vector representing the data we had before. It's just like if you have a in an r file, or if you're doing information retrieval in Google, Google has to represent each document as some sort of vector of numbers and the vectors, what they actually represent is up to you. So the first ones might be the product number, that might be the price, that might be the profit margin, the meaning of the numbers is to do with the encoding which you as the data miner decide.

Okay, let's look at a clustering algorithm, a very simple one is k means, the k means, well first of all you have to decide how many clusters do you want and lets say we want 3, then you choose at random 3 of the instances and say these are going to be the prototypes for my 3 clusters, and then for the other instances, you see, you measure how close they are to each of these 3 prototypes. And you assign them to the cluster depending on which one is the closest. So thats a number of instances.

This is in 2 dimensional space, but you can imagine if youve got a vector of 5 numbers, that would be in 5 dimensional space. So this is just country versus age, only there's more countries this time, not just Canada and America. And you can see looking At it.

It looks like there's maybe 3 clusters or possibly 2 clusters. There's one little one at the bottom and at a top one big one, or maybe that's 2 with a gap in between. To decide events clusters, what you actually need to do is decide is there a gap? And if there's a gap, it looks like that's a dividing line between the clusters. So they want to choose 3 of these. And you pick 3 points in theory at random. There's one, there's one, and there's one. So we've chosen 3 points, and we say these are our prototypes. We're going to assume initially these are at the center of each of the 3 clusters. And if theyre at the center and all the other points, we measure how close they are to these 3 points. And we can partition the dataset by saying, all of these points are nearest to that one, all of these points are nearest to that one, and all of those points are nearest to that one. Therefore we have 3 clusters. And then hey presto, you've got 3 clusters straight Away.

But that isn't a very good clustering because you can probably see these Lot.

Look as if they should belong to that cluster. They shouldnt belong down to that Cluster.

So we then we iterate, we say, okay, assuming that is a cluster, now we don't have to rely on that being the centroid. We can actually take all of these instances and calculate the real centroid of this cluster. And the real centroid of a cluster is a bit different. The real centric is down here, its not there, and the same for the other 2. The centroid for this cluster Isnt that one.

If we compute the average of all of those points, xy points in the average is actually down here. So we have changed the Centroid.

And now weve got better centroid switch cluster. Then we can recompute the Partitions.

For example, lets go back here. This point here, it started off as being part of this cluster because its nearest to that centroid. But if we recompute the distance between there and there, well see this one is actually nearer to That point.

And therefore it should be the other side of the boundary. So now some of these points have actually moved from one cluster To another.

And we keep doing this over and over again, readjusting the centroids, repartitioning, readjusting the centroid, repartitioning until there's no more change, until none of the points move clusters and then you've got the final Clustering.

Its not still not perfect, theres still a couple of points which look as if theyre in the wrong cluster, but weve done fairly well. It also depends on which points you start off with. If I've chosen this point and that point as the random starting points, then this wouldnt have worked because all of these points will probably have that one as its nearest. All of these up here are probably of those 3, that one is the nearest point. So you just end up with one cluster. So that, it depends on where the initial points are. Okay? That only gives you 3 clusters. If you say you want 3 clusters, it wont give you, it won't let, it won't decide for you how many clusters there are. So if you don't know in advance how many clusters there are, what you do, well, one thing you can do if you think it's going to be 4 clusters but you're not sure is run the algorithm for 4. And then run it again for k equals 3 and for k equals 5 and maybe for k equals 2 and k plus 6. And just examine all the different results and look to see which one gives you what looks like the best. Clustering. Unfortunately, were clustering, the only way of evaluating is what does it look good? Its not really the same as accuracy.

Its just, it looks reasonable.

But if you don't know and if you aren't going to want to try all possible permutations, then you can do something else. This is building up for clusters, going through a data one at a time, each new one, and we see which cluster you've got so far. Is it closest to, and if it's close to one of the existing clusters and put it in that cluster. And if it isn't, then keep it out as a separate new Cluster.

And this is based on what they call category utility that is given this new instance and what's the overall distances between the Clusters.

So here we have an Example. The very first instance in your data set, thats going to be a cluster by itself because we havent got anything else to add it to. The second one, lets say its a separate cluster, so weve now got 2 clusters, a and b. The 3rd one comes along and you have to decide c might go with a into a separate cluster, or it might go with b into a cluster or maybe it's different from a and b and therefore should be 1/3 cluster, and you have to use a computation of how good, what the distances are between these clusters to decide and probably because youre only starting out is probably going to be different from a and b and be a separate cluster and then you do it again for d, the same thing happens Again.

And is either in with a or in with b or in with c or a separate cluster on its own and probably since youre starting out its going to be a cluster of its own you keep Doing this With e, the same thing happen and then you get to f and finally you come up with, it turns out that f is very similar to e. So we've actually found a pair of instances which are very similar. Therefore we have our first multi instance cluster. So now we've decided, the algorithm has decided a, b, c and d are all different from each other in separate clusters but enf are close enough together to form a separate cluster. So you can see this is how it works and just keep going. You keep on adding new instances, deciding which cluster to add it to according to how, which one it is close to, any of them and if it isn't close to any of them then you don't add them. And that ends up giving you something like this dendrogram. It allows you to incrementally build up clusters and you can see how similar or how different the clusters are. And whereas the k means it depends on which starting points Which 3 random points you start off with. For this, it depends on which order you give the class the instances to. If you take the set of data and randomize it and do it again, you may end up with different clusters. Okay, so you should have an idea now that classifiers take label data and add each instance to one of the existing classes. If you haven't got labeled data, then you can use this unsupervised learning to either find associations between features or clusters. Find you natural groups Or clusters.

So you may, you should be able to compare classification with association rules or compare classification with clustering. You hopefully now know what is the difference between supervised learning and unsupervised learning. Anybody not know what's the difference between supervised learning and unsupervised learning? Good. You all understand? Okay. I knew if I asked a negative question, nobody likes putting their hand up. So no one's going to admit to not Understanding.

Let's try the opposite. Okay, who here understands the difference between supervised learning and unsupervised. Learning?

Tell everybody. You should now put your hand up. Logically, you didn't put your hand up before, so now you should all put your hands Up.

Who doesn't understand the difference between unsupervised learning and.

Supervised learning?

And who does understand The difference? Okay. Who isn't here? And There's quite a few of you not here about to say, that's The onus. Okay, well, well.

That's, that's a, I mean, if this was a bigger machine learning course, you might have more on this. But actually this sort of stuff isn't very widely used in practice. Most of machine learning is about classification rather than clustering and association rules. And the classifiers work because you can, even though hand labeling data is expensive, it is fairly straightforward to automatically generate labeled data in various Clever ways.

Okay. The second part of this is about chat bots and as you probably gathered by now, im very keen on chatbots. Its a fun thing to play with. Ive just been on a a UK a I. Conference call this morning about what can we do about chat gpt in our systems because everyone's gonna use them for cheating in their assessment. So it's a very topical thing at the moment. Okay I imagine youve come Across Various of these things. There are essentially 2 sorts of chatbots. There's your general chatting to them type things like just for playing music and having fun. And then there are also if you like practical commercial applications for answering questions and if you're having a general chat, like with chat GDP, it doesn't really matter if it gets it wrong or not because you're only having a conversation. Whereas if you're trying to book your tickets for a plane or a bus or something, then it's got to get it right. It can't make up stuff because you'll end up going to the station or the airport and it's not there. And so it, for conversation agents, theyre just for fun. For taskbased dialogue agents like booking flights, it is important that the answers are correct and appropriate. And there's also 2 sorts of architectures. The rule based system. So in AI, generally, there's 2 sorts of ways building AI systems. One is for the, to write a whole lot Of rules.

And then the human has the expertise encoded.

In the rules.

So if you have a lot of, if you could come across a frequently asked questions website, then you have a lot of questions and answers written by people and then the chatbot simply learns all the questions and answers. And when a user types in a question, the chatbot looks up the user's input against the database of questions to find a nearest match and then it spews out the answer that was in the data set.

The other sort of approach is that corpus based training to have a large data set of texts and to feed it into some sort of information retrieval neural network system and then the system went at used runtime the user types in something and that whatever the user inputs is. Put into the neural network and the neural network generates from the combination of user's input and its knowledge of all the texts it's seen so far it generates some new output. So this is an example from blender bot which was like an earlier version of chat ttp. It was a large neural network language model and you can cite whatever you like.

Will you sing me a song and it takes in the sentence will you sing me a song? It and it somehow it combines that with the network knowledge of the English language to generate an appropriate English sentence as a reply.

Sure. What do you want me to sell? What do you want to be sung to? What do you? Sorry? Sure. What do you want to be sung to? I can sing you a song about Baking.

Okay. That to me, that doesnt sound a very grammatical sense. Okay? But It works Or in China.And do we have any Chinese speakers here? How do you pronounce this name? Xiaofei sick sit Xiao chair. Anybody heard of this? No Chinese Speakers? Shopping. Shouting. Shopping.
Okay. This is not a Shocking.Anyway, the point, it looks like it's shall Ice. Shall ice. Anyway, then the Chinese ice is. In Chinese ice.

Okay, so in this system would, for some reason Microsoft released this in China and in Japan and Korea, but they never released it in the UK or America as far as I'm aware. And this is a very general chat system. It works, I mean, it doesnt really, it doesn't really, it's not there to help you buy tickets or to tell you about how to pass your exam. It's just to have a chat too. But it apparently was Very popular. Has anybody used this system?

Anybody chat? No? Okay. And but it is for a task based system. If You have to Come up with an answer which meets the user requirements, then you have to have a much more Sophisticated Framework. You can't just have a neural network capturing everything in some abstract network so that to answer questions about, for example, booking train tickets or plane tickets, you have to know the things about the the origin, the destination, departure date, arrival Date.

The airline, this other stuff. So if you're going to build A chatbot, you need to make it.

Appear to be human, which means you have to know about human conversations. So there's lots of research going on to transcripts of human to human conversations. For example, this is a person talking to an agent, a travel agent and a human client C.

And one thing you notice straight away is that they take turns. Somebody says something and the other person responds and so on. And the turns can be very straightforward, like you can say, OK, and OK is a turn. It means you've got it right so far, carry On.

But turns could be very long as well. But it's not simply turn, turn a, turn b, and so on. But you can have interruptions. So here's an example. Some, some, the agent says, okay, there are 2 nonstops. And then the client says, actually, what day of the week is the 15th. So he stopped him because he doesn't want to hear about 2 nonstops. Maybe he wants to go back and change his mind about something else.

And that's called barge in that the user interrupts in mid speech and that things like Alexa have a real problem with that. If I, if Alexis telling me an answer expects me to listen quietly until it's finished and then I can ask another question. Whereas in human reality people do bargain all the time. Theres also this issue of endpoint and you have to know when you finish speaking or when the other person has finished speaking. It may be a pause so that it indicates you can Stop now.

And part of this is because when people speak to each other, theyre not simply issuing a sentence and expecting a sentence in reply. Theyre taking action. So the language Act. Can be something like a, and acknowledgement. I can say OK or yes and that isn't an act. I'm not, when I say yes or OK, im not saying, im not issuing a question. Im simply acknowledging that what you said so far is okay. Oh, thanks Is another one.

This another very important issue is grounding. That while I am having a conversation with someone else, both of us have to keep being aware of what the other person knows while.

I'm Answering your question. If you ask me a question, I assume that you want the answer. I assume you're not simply saying this out Of mid air.

A very simple example of grounding is when you use an elevator Or lift.

In Britain, you press a button and you just press a button and go to the next floor. When you press the button, you expect it to light up. And this is the elevator telling you that it acknowledges that you've pressed the button if it doesn't light up. But I keep pressing because I'm expecting it to tell me that he understands I want to move. I think there's something wrong if it doesn't light up. So the elevator is acknowledging that it has been called, even though that's not actually the function of the elevator. The elevator is just supposed to take Me up and down.

So when you say, okay, that means I, I understand. I'm telling you that I understand what you've said so far. Well, here's another one on the 11th. I just heard you say on the 11th. Is That right?

Okay. There's lots of examples of this. So there's an overall structure of turns, and you can even have subturns like a Correction.

Oh, I, I, I, maybe I was wrong about that. Or a clarification giving more information. Or can have a presequence like the user actually wants to reserve a seat, but before he says, can I reserve a seat, he just checks that you can make a reservation. And that's not, I mean, he says, can you make train reservations? That isn't what he wants to. He doesn't want to know if a system can make train reservations. He actually wants to make a Reservation.

Theres also the point about whos in charge in a conversation. When im talking to another person, well, if im talking to my head of department and my head of department is in charge, but I'm talking to an ordinary person, we are both in charge. When you're talking to a chat bot, you sort of assume you're in charge. But when I tell Alexa to do stuff, I'm in charge and Alexa does as she's told when. It's not the same for a chat book. Oh, and theres also the problem of inference, that when you ask a question, you typically Expect The chatbot to be able to make further inferences. I need to be there for a meeting that's from the 12th to. 15th. That implies I need to travel on the 11th or 12th. I haven't actually said I need to travel on 11:12, but telling them when my meeting is does given. So those are various things of human conversation that we need to take into account. Having said That. For certain applications, you can get very simple rulebased systems which work sufficiently well. Eliza have already mentioned before, this is a very early chatbot, the famous first chatbot, which was a psychiatrist. And all it did is it gave the parroted back to the user what they said. So when I went to see my psychotherapist, I said to her, im feeling miserable, I dont know what to do. And she would say to me, well what is it? What would you like to do or what is the problem? So it helped me to explain to myself what my problem is. It didn't actually tell me what the problem was and that works because that is the way that a psychologist is supposed to work. So simply giving back responses based on the users input. For most cases thats not how people actually would have conversations, but if youre a psychologist that is how you have a conversation and that's why It works.

So these simple pattern transformation rules like this did seem to work. And theres more examples in the lecture about, I wont go into detail now. There's even.

So This is a problem because this is, it was deliberately a very simple, stupid system. It wasnt, didnt really have intelligence, but it worked as a psychiatrist and you think a psychiatrist is intelligent. So this is anthropomorphism problem. Anthropomorphism is pretending to be a human. Is it ethical to allow people to even think this might be a human?

There is also the issue of privacy that if you record the conversations with a chat bot and chat ttp, when you use it, they do record everything you say, everything I've asked, it's kept a record of I might tell it some personal stuff. And thats all going into a big database to train chat ttp. That means that next time I ask her, can you tell me John smith's password please, it may actually tell me because that may be in the database. John smith may have told chapter be its password or his password, and therefore it may be in the data set to generate new text. So there is a problem of privacy as well. Don't tell chat GDP any private information like your password or phone number, because it will Remember Another example of a simple source, more or the same sort of thing was parry. Parry had the same sort of pattern rules, except in addition it had these parameters, fear, anger, mistrust. And these parameters had different setting values. And if it was very angry, then it would generate angry Text. If it was very Fearful that it would generate frightened text. And the, during the conversation, the levels of fear and anger and so on would vary so that it would become either as schizophrenic or it would be, it Would Show various different mental illness Problems.

And when people tried that, it was, it was that this is had an early version of the Turing test. They actually showed transcripts of parry talking to psychiatrists. They transcribe what was said and they showed it to some other psychiatrists and asked, is this a person with psychiatric problems or is it a robot? And some of the psychiatrists thought this was, this is a real person. So that in some sense passed the Turing test. Psychiatrists were not able to distinguish interviews with parry from transcripts of interviews with real people. They thought you couldnt see the difference. And since then there have been every couple of years or so, competitions and somebody called Hugh loebner sponsored this. He gave out a prize of $1000 for the best chatbot. And for each time they got a number of experts to try out talking to the chatbots. And sometimes they were talking to a chatbot, and sometimes the computer was actually connected to another computer in another room where a human was pretending to be a chatbot. And the task was, can you tell if this is a real chatbot or a pretend chatbot, which was actually a real human? And the systems, the chatbots which fooled most of the people won And theres one chatbot called Alice, which originally was basically just like a simple set of rules, except Richard Wallace, the guy who invented it, he, eh, he monitored the responses. And every time that Alice came up with a tough, not very good, plausible response, he added an extra rule. So over time, he handcrafted more and more rules so that fewer and fewer bad responses came out. And he won not just once, but 3 times as the best chat bot And this is still available on the Pandora box website if you want to. And you can take the original language model and add in more rules if you want to make it to personalize it. And they for limited domains, this actually is these rule based systems will Work.

We did have last year a chatbot called Hubert as part of a European funded research project. And the Hubert asked that was I asked all the students to have a conversation with Hubert. And Hubert asked, what did you think of a lecture? Was it, was he a good lecturer and what would you improve and what would you get rid of? Those sorts of questions.

And then the chapel would say, are you sure? Isn't there something about him that's good? So it would draw you into a conversation and at the end of it, maybe you convinced that it's not so bad after all. Unfortunately, the company who did this, and they decided to stop working with universities because there wasn't enough money in it. And they've changed. So they're now developing chatbots for interviewing job applicants. I think I've said This already.

And we, this is quite a while back, she's now a lecturer in the gulf somewhere. We write a paper on chatbots, are they really useful, which has a number of examples of practical applications of chatbots. Hubert was one of them, but it's no longer available. If you read this paper, there's some more, the 3rd lecture in the Series is So some more information about that It's how do these work?

As I said, there's either by information retrieval or by generating.If you're going to For both of these, you have to have a corpus of texts, and you can use something like the British national corpus if you want to. Bayan worked with the corpus gesbroka Africans, anybody who speak Africans?

No, from South Africa, that people in South Africa collected a lot of conversations and we used their corpus to train a chatbot and then we put it online, and then people in South Africa were able to have a chat with it and tell us if it was any good. Obviously we couldn't do it because we don't speak Africans. But essentially if you have a corpus in a particular language then you can train the chat bot and then it will chat in that language. And if youre going to do.
That, its The easiest method is the classic information retrieval method. If you have a corpus which has turn and then somebody saying something and then a response to that. And as well, if you have a corpus of conversations, then when the user puts something into the chatbot, you just find a nearest match in the conversations and then you spew out whatever response was To that.
And that works reasonably well. But there is the alternative is the neural network method, which when a user types in a Question.

The corpus is used to train a generative model in the neural network. And then at runtime the user types in something, the user input is input to the neural network and the neural network generates a response. So there isnt a set of patterns directly. All the patterns are encoded in the neural.

Architecture.

Thats what Burton dpt and stuff.

Like that were Name. It's by generating the most probable sequence of words from a chatbot. There is a problem with this that if you generate the most probable.
Sequence of Words, then you may end up generating the same thing over and over Again.

Or is another example, and they often say more or less the same thing over and over again because that they're supposed to generate the optimal response to whatever was input. And theres even more sophisticated methods. So chatbots can be Fun.

They're very good for very specific applications, but be aware, they dont really understand what youre saying and they can make up stuff. When I ask chat ttp about who is Erica university, it told me lots of stuff which isnt true, which is very impressive. I was president of the European association to computation linguistics, and I know there is no such thing as the European association of competition for 6, so I can't be as president, but it any sounds good. So I'm thinking about putting on my web page, but advised not to anyway. So you dont trust them. Thats the real problem. So if you're going to use chat gtp for your coursework assignments, you are still responsible for Checking Whatever it says is true And correct.

Finally, if you're going to use nowadays, chatbots aren't just on their own. Another big difference is chatbots are now usually part of some larger system with access to backend databases and all sorts. For example, at Lancaster university, they have a chat box for students called ask lou. And you can ask when is my next lecture or how do I book a meeting with professor wrayson? And it can only answer this by accessing the timetable database and the university map and other backend stuff. And that those are not text databases for or text corpora for training. And being part of a larger system makes it more Complicated.

That's enough on that. One final thing before we go I want to mention is it's very hard to evaluate these, apart from just saying it Seems good.

There are other metrics for evaluating and theres also ethical issues, but and various things. If youre relying on the chat book and it makes a mistake, what happens then? Also it's often biased in various ways. So you have a PhD student looking at bias in education. If it's based on, if it's Learned from a training corpus, if there's bias in the data, in the training corpus, then it's going to be wrong, going to make all these all sort of mistakes. Tae, for example, this is a Twitter chat bot launched by Microsoft and it was trained on Twitter.

And the problem is after a very short amount of time, it started in Twitter, there's lots of Nazi propaganda and the conspiracy theories and harassment of various people. And it Learned all that from the training corpus and therefore went on to start harassing people and convincing them to be Nazis and stuff like that. So that didn't work. It depends if the training data is.

It's data mining and text analytics, but, turn this off. So ive just come from one of my PhD students is having her PhD viper at the moment. She started at 1 o'clock, so ive been watching, doing fine so far. I've had to cut, break away to come and get this lecture. At the end of lecture, I'll go up and see. Maybe she's finished. Hopefully, yes. Anybody enjoying this topic, stay on to a PhD next year. Why not? It's Much more fun.

Okay, so youve had a chance last week to catch up on your, this last week it was On Starting work on the group project and doing the test. And the test scores overall were pretty Good.

Now, remembering that this is the grading system is if you get 70%, that's 14 out of 20 or above you've got a distinction. So don't know I'm not expecting nobody actually got 20 out of 20. If you got a couple of the questions wrong, dont worry about it if you got 14 or more, thats really good. Thats amazing thats distinction whereas pass is half or 50% or 10 out of 20. So most people nearly everybody got somewhere between 10 and 13.

But a few people a handful failed a handful got distinction on the end. Remembering also this is only one test is another test coming up, but most of the Mark comes on the final project thing, so even if you didnt do brilliantly on the test, you still got a chance to do very well on this project that you were working On. Im not going to go through the questions in the test one by one, because you dont really want to learn the specific answers to those. But rather what I'll do is near the end when before the final test, ill go through what sorts of questions, what types of questions people found difficult. So typically if there's 3 possible answers and you got 2 of them right but the other one wrong, then you don't get a Mark that was promises is bound to be to make sure that it is of the appropriate level of difficulty, so that most people get passes or Merits.

Therefore, there have to be some very easy questions and some very hard questions because otherwise people would, everyone would be getting 20 or 20 and we're Not allowed to do that.

Well, look at some of the question types later on, but not Now.

What I want to do Today Is to summarize some of the key points that were covering this week, which I think is on machine translation, information extraction, and also some python Tools.

How I arrived a little bit early. I would have given you a link to something. Maybe if it's time at the end, I'll show you some more about python tools. Okay, let me have a quick, see if I can. Normally I try to download these things before We start. Let's see if I'm down on top one. And. Download that one. Okay.

That's more again. Make that smarter again. And with your logo, this one. Okay. Notice that the lectures you should watch and or you can look at the slide if you want to watch the lectures.

There Presentations by experts in the field. So ive just reused their presentations if you, these are sort of overviews of what the problems are. This is by nizza habash. He is a professor at New York university. He actually leads the camel lab, which is computational analysis Of Middle eastern languages. I think that's what stands for. In other words, it does Arabic. So if you want to do any Arabic language processing, look for the camel tools. He's actually moved to New York university, Abu Dhabi, cause Abu Dhabi is a good place to do work On Arabic Better than New York, in New York. And this talk is pretty much about what the problems are. If you want to find out more about the current neural network deep learning methods for doing machine translation, then there's the textbook chapters. I have included.

Here are Some. What I'm going to do is a quick overview of everything. You want to find out some more detail than watch the recording of each lecture but for more technical details, then you can look at a textbook chapter. There won't be any questions in the final test on the details of the neural networks in the text in the textbook. However, if you're thinking of doing a research project either for the assessment or for your MSC project involving machine translation, thats basically where you can find out more detailed information. So in general all of the module is free tune. It is a summary at the end of the week. There's some more details in the recorded lectures and there's background stuff in the textbooks for those of you want to deep dive into that particular bit. Okay, let's Quickly Summarize what machine translation is About. And in particular im sure youve all used machine translation. Google translate works reasonably Well.

Just reminds me divide 1/2 an hour ago the professor of Arabic was pointing out some mistakes in the student's thesis because she had used Google translate to translate bits of the Arabic quran into English and he didnt agree with her translation. So Google translate isnt perfect but it gives you the gist. It can figure out more or less what it means even though its not really the ideal translation. So humans are better. Also although it deals with lots of languages it relies on training data and many of these languages there are there isnt much training data, for example, if youre translating from Esperanto into Latin, for example, and anybody here speak Esperanto.

Not many people speak this, and you have to train it, you have to have samples of the source language and the target language. And so from Esperanto to Latin, there isn't that much translated text available for many of these. English is available to what Google translate actually does. It translate for Esperanto into English and then from English into Latin. And that makes it, that's another reason why it's not necessarily as perfect as a human.

And just for fun, I put in this slide for another, this isnt machine translation, but another project ive been involved in is something called world mapper. And its just, its a computer graphics project which can draw a map of the world where each country is either expanded or contracted according to some ratio, which you supply. And in this case, the figure, the ratio that supplied is the number of Arabic speakers. So the countries which are big because they have a large number of Arabic speakers, and the ones that are small out have few. For example, a big orange blob in the middle, thats Egypt because its a lot of Arabic speakers in Egypt. And just below Egypt we got Sudan, which is quite a lot of Arabic speakers in Sudan. And then the squiggle at the bottom, the tail, thats the rest of Africa, because there arent that many Arabic speakers in the rest of Africa. And you can see America very squashed because theres not many Arabic speakers In America.

Okay, in terms of machine translation, I did say the technical detail is in the textbook, but what are the Problems.

And what are the general sort of methods that we can use for doing machine translation? And then how do you decide if machine translation is good or bad, or if youve got 2 systems, which one is better?

Well, an obvious problem for many languages is theyre not written using the same alphabet, so youve got to deal with converting the alphabet. Actually, in computing terms, this isn't really much of a problem because what you have to do for any text is take the characters and devise a mapping scheme from characters onto numbers. And for English or for the Roman alphabet or the European alphabet. And that's fairly standard. There's a standard mapping for each character. It has a particular numeric code. And once its represented as a series of numeric codes, then the same algorithms will work on numbers. So thats not a problem. And there are some further problems in that, for example, in Arabic, vowels are effectively optional. So you can decide whether or not you want to include them. And that means that 2 Words.

One written with the vowels among it and without the vows should be the same word, but actually arent the same character sequence. So that makes it more complicated. Or for Chinese, any Chinese speakers here? Okay, you can tell me if I've got this right. In Chinese you don't have word boundaries. So obviously you have characters, but sometimes the word is 2 characters or 3 characters, depending on how you analyse it. And this also is a problem for mapping from one language to another. So one word in English may have 2 different senses, which are different words in Arabic. Or bank, for example, is either financial bank, like the way you put money, or river where the water flows. And there are some words in English which you dont think of as having 2 different senses, but in the target language they are translated differently. So eating is fairly, it just means one thing, you put things, you consume. Whereas in German, for a German speaker, a human eating is a different thing from an animal eating. And you have to say if Esther if I'm a human or if Esther if I'm a cow, I dont understand. To me that doesnt sound reasonable but there are different distinctions in different Languages.

And the same thing happens, the same problem at the level of words is a word, an Arabic speaker can tell me if im pronouncing this correctly, I think is one word in Arabic, but in English that means and the cars, which is 3 words and in friendship is also 3 words. So it depends on how you divide up sentences into words is not the same, which means you can't do word for word translation because they don't line up properly is another example. In Arabic, I am not here is 2 words In English, I am not here is 3 words. In French, junior sweeps, 4 words. The same phrase is chopped up Differently.

Even more complicated, the core or pivotal meaning of a sentence in English is usually the verb. So John swam across the river quickly. If you translate into Spanish then you wouldnt say swimming becomes a sort of adjective. Youd say Juan huzo papidamente errio nando means while he was swimming. So John crossing becomes the main focus of the sentence. Or in Arabic, the main focus is going quickly, speeding, John cross the river swimming. Or in Chinese, it Arguably it's Swimming across, isn't quite the same thing. So what is the focus? What is the main verb difference? Differs in different languages because thats part of The problem.

The computational problem is youve got to get hold of resources to build the systems. So one thing you obviously is a dictionary of the words in the language, and if youre translating from one language to other, you have to have some sort of reach word in English, what is the French equivalent and sometimes theres 2 words and you got to decide which one it is depending on the context. So you really need a corpus.

But Not just a collection of text but a parallel corpus, a collection of English sentences and a collection of freight sentences and they have to be aligned for each English sentence which is the equivalent French sentence or if you're doing English to Spanish for each English sentence what is the equivalent Spanish Sentence. And some languages there just aren't that many parallel Corpora Like amharic, the main language in Ethiopia And. Luckily. The European union and the united nations and various other international organizations have this policy that everything has to be available in English, French, German, Hindi and Arabic and Chinese. I think thats the united nations languages. So there are lots of parallel corpora for legal texts for united nations in those languages, but there aren't for others. So that's some of the problems.

How are you going to do the translation? Well, theres sort of like 3 levels of abstraction. The most straightforward thing is to do word for word translation. And the early version as a Google translate did that for most language pairs. So you take the English sentence and you change each of the words using a dictionary into its target language. And that gives you the gist. A gist is the, the, an outline of what it means, even if it's not a perfect translation. So here we have a Spanish sentence. Anyone who speaks Spanish.

Okay, im going to mispronounce this and no one can tell me otherwise. Established in 1988. I can't say in Spanish, una metallurgy. It wasn't very good. And if you translate each of those into their English translation, then you get a perfectly reasonable English sentence. Envelope her basis out, speak experiences, then settle at 1988, one.

Methodology.

And you all understand that, don't you? Yes, think about it. Well, the problem is word for word translation doesn't really work. It means something like on the basis of these experiences, a methodology was arrived at in 1988, and some of these you can get because the translation, let's go back a bit, so play means envelope, but it also means on. There's some Spanish words which have got 2 different translations in English, and you've got to get Right one.

And its also the problem, whoops, lazy. So if you get the right word, thats okay, but theres also the problem of different word order, so the Spanish has establicio, which means it was arrived at and that was moved around, and methodology in Spanish was at the end, whereas in English it's got to be at the beginning. So you have to know to translate these things around, but if you can do that, so you can translate word for word and then move things around a bit, and that Works.

But if you got to move things around a bit, thats because the grammatical structure of the 2 languages may be different. So maybe a better way is to take the English sentence, work out the grammatical structures, convert those grammatical structures into the target language, and then generate the rest of the sentence from that.

So here's a simple example, this is Spanish into English, and the Spanish says x, whatever x is, some puzzo mantekia n, which means put butter on. And in English you don't put butter on toast, you just butter the toast, right? So the translation of that phrase is a simple sing single verb In English.

So that's called transfer. That is taking the grammatical structures and transferring them into the target syntax before you then generate the target words. But really what you want to do, what we humans do and what deep learning systems do is they represent the meaning of a sentence. And then using that meaning, they Generate.

The sentence in the target language. So that's what gpt for example, it learns that when you type something in, it understands what you said but enough what you typed in, it comes up with an internal representation of meaning and then it generates an appropriate response. In chattees b does it in English, but you can imagine a gpt translator doing it in French or something instead. And that the early ways of doing this, we didn't have neural Networks.

When I was a boy. We just had other Methods. Well, even when I was an undergraduate, they didn't have a deep learning much. So what they try to do is to work out sort of logical representations of meanings. So here's some examples of John broke into the Room.

And broke is something like cause to be broken forcefully or something like that. So that people working out knowledge representation methods, lot logicians for philosophy try to work out logical representations of the meaning. And then from the logical representation can generate an equivalent Spanish.

And that was from 1993. Nowadays we dont do this anymore. Rather, neural network, you feed in an English sentence and it comes up with a representation, which is some combination of nodes and weights within the neural network. The problem is we dont really understand how its doing it, but its representing it in terms of weights and notes. The nice thing about this is you can see the structure, so its explainable to someone. But on the other, it doesnt work very well. So that's the interlingual representation. And for each of these you have to have some sort of dictionaries or lexicon. If it's just think that you have to have a wordforword dictionary, if its grammatical analysis, then you have to have a transfer lexicon of grammatical structures in English and what the equivalent is in Spanish, whatever it is. And for meaning representations, you have to have a meaning representation of the English and how that maps onto a meaning representation in the Spanish.

Okay So A very oversimplistic view of how this works is, remember I said you have to have a parallel corpus of preach English sentence, what is the equivalent in Spanish if you're translating English Spanish? So there is a tool called giza plus, which is a tool kit you can use, which given a parallel corpus, it will learn trend which words in English are likely to be that the translations in Spanish. Now, a nice thing about English and Spanish is that theyre more or less, theyre very similar. Theyre both in some sense derived from Latin.

But Modified in various ways. So many of the words, some of the words are similar, and the word order is similar. So the English sentence Mary did not slap the green Witch. In the corpus that is aligned with the Spanish sentence, Maria nodio Verde. And does anybody here speak Spanish.

Please? I'm murdering this so that it's probably not very good. Anyway, you can see that Mary, the Spanish word must be Maria. And it does this by taking lots and lots of sentences and it looks up all.

The sentences which Contain Mary, and for all of Those.The equivalent one in Spanish, what word appears in them. And Maria is the word which appears in most of the sentences which are the Spanish translations of English sentences containing Mary. So it does it by essentially statistical Inference And the same thing for all the other words like the words slap. If you look at all the sentences which have got slap in them in English and look at the Spanish equivalent ones, then many of them contain something like deal, owner, buffatada which is 3 words long but therefore it must be the equivalent, must be the Translation.

And the diagonal is pretty much the same, because that shows that theyre more or less the same word order, except at the end, green appears in sentences which have got their day in the equivalent Spanish. Therefore, green must be the translation of Verdi, and which the sentences containing which have got brew in the Spanish one. Therefore, brouha must be the Spanish for which.

But even though theres the wrong way round, you can still extract these meanings. And that means IBM, for example, came up with a word for word translation system, which basically does word for word translation and then has some special rules for moving things around a bit. And that works very well. So we have fertility bell insertion, translation and distortion.

There's 3 different sorts of tweaks to make it better. Having worked out, for example, is actually green, which the order of the words is wrong. It can be useful to look at not just individual words, but whole phrases. So thats the a better way of doing translation is if you can find phrases which map onto other phrases, then once you spot the whole phrase, you don't have to look at translations of each individual word, you just translate the whole Phrase.

And that's much more accurate. You're much more likely to be. So how you can do this is you've already worked out that Mary must be Maria and slap is deal owner, buffetado and so on now youve got that then you can work out That. Slap the must be do your own buffeta for example, or Allah or even bigger Mary did not is Mariano or green witch is brujaverde and so on so you can carry on building up larger and larger phrases as being translations of the English so to the level of Mary did not slap becomes Maria nordio on a buffalada. You have to keep in your dictionary all of these phrases that makes it prediction that gets much bigger, but it means that when you come across a new sentence which says Mary did not slap the blue whale, then as long as you've seen Mary did got slap before and therefore you can just translate that directly as Marianne or puffatada. In fact, you can Have the Whole sentence in your dictionary. So you can have a dictionary of lots of Sentences.

And this then works much better. But it does mean you have to have a lot more data. The more data, the longer the learn phrases. If you've got lots of data that you can afford to learn entire phrases. Okay, how do you decide if youve got 2 different machine translation systems, which one is better? Or typically, for example, here at least university, weve got at least a dozen different machine translation systems and you want to figure out which one out of 12 is the best? Well, it's not really, it's more of an art than a science. It's not so obvious how you do this. Typically what you do is you get some people who are expert translators and you get them to look at the results of the system and say which one is best. That's about the only way it looks good to me is the way to Evaluate it. And there are some sort of semi automatic ways. We'll have a Look at a minute.

But humans can be very slow at looking at lots of examples. We would like to have some automated way of doing it because you can speed it up, you can come up with a metric him subway if you're measuring how good translation is, it's not just a score, but there's 2 things to measure one is accuracy and typically this is a score of 1:5. 5 means you've the output in Spanish captures the meaning perfectly of the English Input.

Whereas one means it doesn't but theres also the problem that the output has to be intelligible, has to be fluid has been natural Spanish so if it gets a score of 5, that means it's very good the nice thing about measuring fluency is you don't have to you don't have to be a translator to be able to measure this to measure fidelity or accuracy, you have to be able to understand the English and the Spanish to check that they are correct. Whereas if you're just measuring is the output Spanish good Spanish, then you just have to be a Spanish speaker or if you're translating for Spanish to English. But I can measure the intelligibility of the app, but I can see that Google translate isnt translating very well, because what it gives me doesn't make sense, even if I don't understand what the.

Original was.

Ideally, what a system which scores well in both cases. So IBM came up with this blue score, which is a sort of way of not it semi automating in that it gives you a score, whereas the human just looks at it and says it looks good that's not really a score as just says I like this one and not.

That one.

And which is a bit sort of subjective and not formal enough so what you can do is if you have a test. Set of sentences and you translate them and then you back translate and this is a back so colorless green ideas sleep furiously. A very famous sentence invented by Noam Chomsky to illustrate the challenges of natural language processing. What does it mean? I don't know colored color screen ideas features it means all sorts.

Of things.

If you translate that into another language and then translate the translation back into English then you get a number of translations depending on different interpretations of this sentence. So all double Jade ideas sleep.

Irately.

That seems even less plausible but its grammatically well formed and it probably means the same thing. So what the blur metric does is it festival says from a test sentence which words appear in the translations and 4 of the 5 words appear somewhere in the translations. Therefore you get a unigram's score of 4 out of 5. Which of the diagrams occur? So there are 5. Sorry there was only 4 diagrams because there's 5 words and therefore there's only 4 pairs of words in there. And all these pairs of words 2 of them occur. Therefore the bycram position is 2 out of 4.

And then the blur score is that they tried various different equations and to figure out that the one which gives the best, best comparative score is this one. Its, no point 8 times not point 5. The square root of is not point 6,3,25, which is a score of 63.25%. And 63 doesnt mean anything of itself. However, if you score, if you got 2 machine translation systems and one's gets to score 63 and the other gets to score 65, and that means the 65 is better than the 63. And they tried this out on humans and humans generally, the effect, the ranking given by humans was also the same as the ranking given by this score. Therefore, it's not the score that matters, but rather the ranking that gives. So that in summary, thats one method. There are other methods too, but this is gives you an idea of how challenging it is. In the end, what matters really is which one the customers like best, and we cant tell in advance which one is Going to be.

Okay, that's machine translation. Any. Questions?

So now you understand machine translation. Then next thing, this one, this one. Okay, the next thing I want to show is information extraction. So this is Another Fairly common use Of Text Analytics.

This is based on a presentation by my friend Diana Maynard. She's a lecturer Sheffield university. If you Google Diana Maynard, you may see that she's also does ballroom dancing. So some nice videos of Dinah doing this Sort of stuff.

So information extraction is very useful. We have a look at what it means. This is also a nice example of AI, which isn't necessarily always machine learning. So you probably heard about deep learning systems and stuff. And most AI nowadays does use machine learning. But there is another way of doing this, which is to use engineers to write the rules. So the system behaves in an intelligent way, but it's based on rules.

So we look at some rulebased methods for doing information extraction. So information extraction means, well, getting the names of places and people and organizations from a document, because that is often the most useful bit in a document. Something like Google does information retrieval, you type in some keywords. Well, first of all, you think, what do I want? And then you think of some keywords which capture what you want. You type those into Google and it finds some documents. You then have to read the documents and decide, is this what I want? And if is, then you're lucky is it isn't and you have to think of some other keywords and try again. Information extraction, once you've got the documents, will find the names of the Places.

And this depends on what the task, it depends on what you're trying to do. But you can imagine some things, sometimes you're looking for something and it's not obvious what the keywords are. Like where has the queen visited in the last year? Well, I might search queen visit something, and then I might get documents about the pop group queen rather than the queen of England.

So what would be nice is for the system to extract the names of the places and the dates and the person, queen Elizabeth, and then I'll be clear about I've got the right Documents.

That's so information extraction in some sense extracts the knowledge at a deeper level than just finding the documents that you want. It's more useful. And furthermore, once you've got these names of places and facts and names of places and people and so on, you can put them into a database and then you can query the database and you could be much more trusting of the database. The knowledge is explicitly in the database and then you can put a query to database and get the results out. And in addition, it can tell you which documents it got these from. Whereas the neural network like gpt is trained on lots of stuff. If you ask a question and it comes back to an answer, you don't know to trust it because where it got the information from, you can't see which web pages it used to get this the this Answer from.

So this is particularly important for applications where it is important to be sure to trust that the results for example access to news if you want to find out, for example predicting stock and share prices if theres bad news about a company then their share prices will drop. You want to be sure that you're confident not just finding you have a new story, but where the news story came from or for scientific information if youre in pharmacology or genomics, if youre building some new treatment for a patient for quickly disease, you have to be confident that the information is given you is valid, it's from proper Learned journals rather than just somewhere on the web. So The number of examples, one of them is the health and safety information extraction system. When the health and safety inspector comes to the school of computing, they ask things like how many members of staff have died or had accidents last year, luckily nobis died, but we do have accents. Whenever there's an accident, it has to go into a report and there is a large document with all this stuff in. It'd be so much easier if you could query the document using information extraction rather than having to read through it all. So it finds all the information, put it into a database and when the inspector comes and asks the questions, we can just query the database and it pulls out the information. Is another example is the kibbutz information management system. So this illustrates kibbutz is a sort of farm in Israel.

I think and its got lots of new stories that is found to on stuff to do with Kibbutzes.

And then you can ask a question about by essentially putting an SQL query, but rather having to learn SQL, you can type in something in this nice interface. So you can search for x where x is a person which name is unknown and x is involved in y. And y is an event which name contains kibbutz attack and why took place in zed, where zed is a country which name is exactly Israel.

In other words, I want to find a person involved in a kubutz attack in Israel. Its a bit of a long winded way of saying it, but it means you can ask a sort of question and these are the results. And you can check the results by just looking at the title or if you want, you can click on it and find the full document. So Israel kabut's attack suspect arrested. Well that clearly the suspect is a person who was potentially involved in a kibbutz attack in Israel. Another example is the threat tracker, which Doesn't track.

People using cameras, but rather it tracks people in newspaper stories. And people in particular can have, as the same person can have different Names.

Doctor amash was also known as huda amash was also known as huda sali. Madhi amash was also known as Mrs. Anthrax because the American government claimed that she'd been developing anthrax weapons and to fight the invasion. Anyway, she had various other names and this system knew all the different names and was able to trap even that. The example her, it noticed her refers to the same entity. So that's what named edge. Your recognition is identifying in a text the names of people, organizations, locations and other stuff like that. So its very important because its useful because thats probably inquirying. Thats the sort of thing youd like To get out.

So how do you build these systems? Well, very straightforward. You have to have a dictionary of named entities. If you want to go anything more than that, then you have to have either the machine learning approach, which is getting together a large amount of text and labeling in the text all the persons, places and other stuff. And that's expensive and type consuming. It means you got to pay people to go through the text, Mark, this is the name of a person, this is the name of a Place.

And you need large amounts of it to learn from. You also need high computing power and other stuff. So the alternative approach which has gone out of favor of it, is the knowledge engineering approach which is to handcraft a rule based system. So the a person looks at the problem and tries to think what rules would I need to solve this? Most of the time and you don't need lots of training data. You need some test data to test your system out on, but not a Lot.

And so it could be much quicker and easier as long as its reasonably well. So.

How do you do this?

So the knowledgebased approach is to say, well, first of all, you have to deal with the problems. One of the problems is variation and ambiguity. Like John smith, mister smith, John, they're all the same person. How do you recognize that? On the other hand, John smith could be the name of a person or it could be the name of a company. If you drive from here to York and you'll pass the John smith factory, which is the factory owned by the John smith company, they make John smiths.

Anybody here drunk John smith? It's a beer we have in the, in, in the, at the bar here.

Right? Okay.

Other words like June, Washington, so on there, ambiguous. So you got to deal with both of those sorts of ambiguity. Theres also the problem. You got to look at the format of the text. So for example, if you get department computing, your math match system at one university, Manchester, United Kingdom, at the top of a piece of paper that tells you this is a letter, and that's an address that the letter is going to or from. So that knowing that something about the format can help as Well.

The simplest approach is to use a set of dictionaries, or theyre called gazetteers. So you have a gazetteer of peoples names, another gazetteer place names, another gazetteer of company names, and then go through a text every to every word. You look it up in the gazetteer, if it's in the gazette here, then you found it as an entity, its very simple and fast. It cant really, there is a problem, you got to collect these names in the first place, but more importantly can't really deal with ambiguity. If it finds John smith, it can tell you its a name and its a name of a person and its a name of a company, but it cant tell you which one it is in this context. You got to look at the context. So at also if in names are really problematic cause names are Often New, words you haven't seen before. For example, Sherwood forest Sherwood may not be in the dictionary, but what you can say is that if it starts with a capital letter it must be a proper name, in English at least and if the next word is something like forest or center or street or road, then it's going to be a place, a location and that works most Of the time.

And there are still problems that names often have to have a capital letter, but it doesnt, its not always obvious which one it is. So cable and wireless is one company name whereas Microsoft and dell is 2 Companies.

So you have to do some sort of analysis of the context. For example ive just said John smith could be a company or a place as our company, or a person. The same for David Walton, the same for Goldberg sacks. Theyre all possibly peoples names or company names. Whereas if you come across something like David Walton of Goldman Sachs and x of Y, the X must be a person and the why must be sense of organization that it belongs to. Because god, the person belonging to a person, that's not allowed. So you can use sketch engine or some other tool like that to show for any of these phrases what comes around them. And that allows you to build up some patterns. And you find that, for example, words like Urns.

If it's 2 entities, x earns y, then the x must be a person and the y must be an amount of money that they earn, or x joined y. X is probably a person and y is probably an Organization.

In the same for left. So you can see for various verbs, these are patterns that come up and you can build these up. So thats the knowledge based approach as opposed to the machine learning approach. You use your knowledge, your intuition, your expertise to build up these rules. And that allows you to have an AI system which is rule based rather than machine learning Based.

So they built this up, the shefford university that is big project gate general architecture for text engineering. And they were able to build this up for English and the English system had a number of different Components.

Like tokenization and sentence splitting, and because that Different Components, each of these things when they try to copy to another language, they only have to change some of the components, for example, the name, the gazetteer of names is obviously different the Spanish, whereas the tokenization rules and the sentence fitting rules are the same. So they still work, they were able to, it was able to expand very quickly to deal with French, German, Romanian, Bulgarian, Russian and even some more exotic languages like Arabic and Chinese and they even tried it out for languages they havent seen before to see how, just to see how difficult it was Chip water. Do we have any chip water speakers here? It's a language from South America, spoken there before the spaniards came and imposed Spanish on everybody. And there's not, how many people speak it, there's not many language resources for it. But it was, it's similar in various ways to European Languages.

Anybody who speak Hindi?

Okay, about this. Hindi Is Challenging for people from Sheffield because it's not written using the same alphabet, but the alphabet is different. But apart from that, it's not so difficult because there's plenty of computational resources. There's, there's millions of people speaking Hindi. Therefore they, it's easy to get hold of text in Hindi. So that is medium difficulty for different reasons. So if you're going to do multilingual, if you're going to do this for different languages, then quite a lot of time an effort is basically just dealing with the different character set. So here's one for Korean and they had to build some some ways of inputting Korean and generating Korean or his. One for Chinese, I think is this is the output for Chinese. But what apart from the input output, the underlying algorithms are still more or less the Same.

Do we have a Chinese speaker here? What does the green thing labeled in green?
What's that is?
Is it what? Sorry?
Yes, a country.

Okay, that is a location. Okay, so thats a location. Thank you. So I can't test if it works or not, but presumably you can see even the input output. But actually for a computer thats a fairly small task. It just its fiddly, but its not computationally challenging. But then the rest of the algorithms work.

Fine.

So that's a natural information extraction. And I said see Jurassic amp. Martin textbook for some more examples because dan giraffe is actually an expert in information extraction. So its got lots more about the algorithms that are Used here.

Okay, that's that. Finally, a few minutes before I knit back to the viva and see if shes passed Or not.

Let's have a look that the last of these talks was on text analytics in python. So in the first couple of weeks, ive encouraged you to have a look at sketch engine And weccer.

As toolkits which you can do research with without having to do any programming. Now some of you may really love programming, but on the other hand, why not use these tools if they're available to do the stuff that sketch engine lets you do by using python code? Well, the downside is you got to write the code. You may make errors or mistakes in your code for sketching and you can.

Be fairly.

Confident that they have, they've come up with the best, most efficient way of doing it. That's how you can download a large corpus and compile it and compare it Very quickly.

Okay, if you want to do it in python, you don't want to start from scratch. There are lots of toolkits out there. So one of them is this natural language Toolkit.

Just said this, all this stuff. So natural language toolkit, this has been around for some time. If you doubt it, it's a free to download package and it comes with lots of text datasets as well, about over 50 corporate and also lexical resources like wordnet, that's a collection of synsex. A synset is a group of words which are similar in meaning. And that lets you do semantic analysis in various ways, that sort Of thing.
And as well as having the software, there's also a discussion forum and various other resources out there. If you've got, if you're stuck, you could ask someone else, how do I do blah blah Blah.

And is even a textbook. Yes, theres an API documentation and everything. So theres a very good thing to do. And its got lots of tools for doing, for example, graphical generation. This is a, the grammatical structure of peer vinken, 61 year old, will join the board as a non executive director in November 29th. And so that's part of the, the corpus is the Wall Street journal corpus. Wall Street journal is a newspaper in New York where they, there's lots of stories about company stuff, of financial information. And so you can download the corpus and you can also, there's various tools for doing things in the corpus like drawing grammatical Pastries on it.

And there are several other python tools. I personally, I'm not going to recommend any of these, but rather what I suggest if you are going to do a project using python for doing text. Notice it's a good idea to look at some of these reviews. These are people who have compared different toolkits. And that is one top 10 python n l p. Libraries 2020. That's a bit outbell, but if you just Google, you dont want to look for python toolkits, cortex analytics, but rather surveys of or comparisons of, because that will help you to decide in your proposal. You might want to say, I'm going to use spacey because, and then you can say, because blah, blah, blah said for this particular purpose, it's the most appropriate one. There are also lots of existing competitions where there are shared Tasks.

So another thing you might want to do is look at for, if theres a competition which is similar to your task, look at what tools they Used And they will probably have compared several and chosen the best one for their purpose. And if theyve used the, if theyve decided, if theyve got criteria for deciding thats the best one, then you can use that too. For example, simmerval every year is about 10 different Competitions.

And you find one thats similar to yours, then that you've got a good basis to work from. Morpho challenge was one I took part in a few years ago. This is the morphologic unsupervised machine learning of segmentation of words into morphemes. So word like unsupervised has to be chopped up into un, super vise and do. And it has to learn this by taking lots and lots of English words and noticing that many of them start with on, therefore un is likely to be an ethics and so on from other things.

So the way I actually did it back in 2005, you can watch the video if you want to. In the early days of YouTube, there was a limit of 10 minutes on your video. So this wasnt, YouTube at that time wasnt good enough to store lectures. Its more like TikTok. You could only have a very short video. So academic set up a video lectures dot net, which I think is still there. If you go to video lectures dot net, you can see lots of lectures and conference Talks Over the past 20 years or so. And some of them are particularly Great.

In a nutshell, this is my presentation to the morpho challenge. Just as an example, I got Andrew. Andy Roberts was a PhD student, and he went off to work for Pearson doing stuff Later on.

So what my solution was that the task was, can you build an unsupervised machine learning system for doing morphological analysis of text. And the text could be in English or in Finnish or in Turkish or other languages. So back in those days, the classes are much smaller. So this is the entire my MSC class that I was teaching at the time. So I said, OK, you lot get into Paris.

And solve this problem for me. That was the coursework. Come up with a system for the conference. So what I decided is get them to do it, do the work and then I would copy them and and but rather than just copying them, I would get them eat put their results together and use a voting system. So 3 of their systems said this is the analysis and one of them said this other thing was the analysis.

And I Will go with the 3 so that's The idea And the program is unsupervised because to be supervised you have to give each example what is the correct. Answer whereas I was getting for each example 4 or 5 different possible correct answer so I still didn't know even though the students predicted what the answer was I didn't know which one was right in principle therefore it still counted as unsupervised learning. In fact its unsupervised learning in at least 3 different ways.

One is that I didnt really tell the students how to do it. They were just left to their own devices to come up with a solution. Therefore they were unsupervised learners. The students were Unsupervised learners.

Secondly, each of the students developed an unsupervised learning program or each pair of students for example, mindan and SAO chaodri developed an unsupervised learning system. Exactly. Mindang is now a teaching assistant in the school of computing. So you if you go to a lab sessions you might.

See him and you can ask about this film.

So that the students were developing on supervised learning and then my system took their results and but did a majority vote. If as I said, it looked at all the results and took the most popular result and that was my result. And again, that's unsupervised because I didn't really know which one was the right one. And this actually worked in theory at least. Unfortunately, some of the outputs from the students weren't correctly formatted. So I had to get Andy Roberts to write a slightly better version of the program. So in the paper, theres my version of the program, which should work but doesnt because the students didnt produce the correct Output format.

And we see it did actually work. So here we have the tables of results if you Look up That end the green blue light blue and gray those are 4 different student results it doesn't go down to 0 so this the the scores aren't as that the gray one isn't as bad as it looks great. Gray one is not brilliant.

But.

Okay, whereas the black is mine so black is the combination of the so the con the majority vote system is pretty much guaranteed to be better or as good as the best of the existing Systems.

And the other colors are even better than the students results these are developed at? Other universities so to prove that the majority vote system work best and. One at this end the dark blue that is a majority vote of the top best university systems so it took the 5 best systems overall and did a majority vote of those and it turned out. To be better than all of them so the very best system is still my system is just im taking different Inputs.

Im taking not just the students outputs but the best of the university outputs instead and theres the same results for Turkish. Sorry this was for Finnish this is for Turkish.

Same work. Okay that's that and I think someone's desperate to come in now so I'm going to.
Stop there. Any questions?

No good those of you watching at home, is a camera up there somewhere if you do have any questions and put them into a discussion forum?
